{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:61: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "WARNING:tensorflow:From /home/zzhang/workspace/interpretation_by_design/.venv/lib/python3.7/site-packages/bert/optimization.py:87: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "***** Model output directory: model_checkpoints/bert_base_seqlen_512_fever_exp_output_rnr_merged_evidences_par_lambda_0.2_no_padding_imbalanced_bce_resampling_pooling_first_learning_rate_1e-05_ *****\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/zzhang/workspace/interpretation_by_design/.venv/lib/python3.7/site-packages/bert/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/zzhang/workspace/interpretation_by_design/.venv/lib/python3.7/site-packages/bert/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "from utils import *\n",
    "from display_rational import convert_res_to_htmls\n",
    "from config import *\n",
    "from losses import imbalanced_bce_bayesian, imbalanced_bce_resampling, exp_interval_loss\n",
    "from metrices import *\n",
    "from tqdm import tqdm_notebook\n",
    "from bert import optimization\n",
    "from bert import run_classifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.python.keras.models import load_model\n",
    "from tensorflow.python.keras.backend import set_session\n",
    "from tensorflow.keras import backend as K\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow_hub as hub\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import bert\n",
    "\n",
    "import tensorflow\n",
    "if tensorflow.__version__.startswith('2'):\n",
    "    import tensorflow.compat.v1 as tf\n",
    "    tf.disable_v2_behavior()\n",
    "else:\n",
    "    import tensorflow as tf\n",
    "\n",
    "\n",
    "# The loss of the multi-task learning is:\n",
    "# $$\n",
    "# \\mathcal{L}_{total} = \\mathcal{L}_{cls} + \\lambda\\mathcal{L}_{exp}\\text{,}\n",
    "# $$\n",
    "# where $\\mathcal{L}_{cls}$ is the loss of the classification task, $\\mathcal{L}_{exp}$ is the token-wise average of explaination loss (averaged cross entropy) and $S$ indicates the length of the input text. The hyper-parameter $\\lambda$ is written as ```par_lambda``` in the following cells since 'lambda' is a reserved word in python.\n",
    "# \n",
    "# The loss function has been changed since 2020.01.08. The old formulation of the loss function tried to balance between cls loss and exp loss, which down-scale both term as well as their learning rate. The new schema respect the global learning rate w.r.t. cls part while one can modify the learning rate of explaination part through $\\lambda$\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "# variable hyper-parameters\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--par_lambda', type=float)\n",
    "parser.add_argument('--gpu_id', type=str)\n",
    "parser.add_argument('--batch_size', type=int)\n",
    "parser.add_argument('--num_epochs', type=int)\n",
    "parser.add_argument('--dataset', type=str, choices='fever multirc movies'.split()) # fever, multirc, movie\n",
    "parser.add_argument(\"--do_train\", action='store_true')\n",
    "parser.add_argument('--exp_visualize', action='store_true')\n",
    "parser.add_argument('--evaluate', action='store_true')\n",
    "parser.add_argument('--exp_benchmark', action='store_true')\n",
    "parser.add_argument('--freeze_cls', action='store_true')\n",
    "parser.add_argument('--freeze_exp', action='store_true')\n",
    "parser.add_argument('--train_cls_first', action='store_true')\n",
    "parser.add_argument('--train_exp_first', action='store_true')\n",
    "parser.add_argument('--exp_structure', type=str, default='gru', choices='gru rnr'.split()) # gru, rnr\n",
    "parser.add_argument('--delete_checkpoints', action='store_true')\n",
    "parser.add_argument('--merge_evidences', action='store_true')\n",
    "parser.add_argument('--benchmark_split', type=str, default='test', choices='test train val'.split()) # gru, rnr\n",
    "parser.add_argument('--train_on_portion', type=float, default=0)\n",
    "parser.add_argument('--start_from_phase1', action='store_true')\n",
    "parser.add_argument('--load_phase1', action='store_true')\n",
    "\n",
    "\n",
    "args = ['--par_lambda', '0.2',\n",
    "        '--gpu_id', '0', \n",
    "        '--batch_size', '2', \n",
    "        '--num_epochs', '10',\n",
    "        '--dataset', 'fever',\n",
    "        '--exp_visualize',\n",
    "        '--exp_structure', 'rnr',\n",
    "        '--merge_evidences']\n",
    "\n",
    "args = parser.parse_args(args)\n",
    "#args = parser.parse_args()\n",
    "\n",
    "BATCH_SIZE = args.batch_size\n",
    "par_lambda = args.par_lambda\n",
    "NUM_EPOCHS = args.num_epochs\n",
    "gpu_id = args.gpu_id\n",
    "exp_structure = args.exp_structure\n",
    "dataset = args.dataset\n",
    "DO_DELETE = args.delete_checkpoints\n",
    "do_train = args.do_train\n",
    "load_best = not do_train\n",
    "evaluate = args.evaluate\n",
    "exp_visualize = args.exp_visualize\n",
    "exp_benchmark = args.exp_benchmark\n",
    "merge_evidences = args.merge_evidences\n",
    "BENCHMARK_SPLIT_NAME = args.benchmark_split\n",
    "train_on_portion = args.train_on_portion\n",
    "freeze_cls = args.freeze_cls\n",
    "freeze_exp = args.freeze_exp\n",
    "train_cls_first = args.train_cls_first\n",
    "train_exp_first = args.train_exp_first\n",
    "\n",
    "start_from_phase1 = args.start_from_phase1\n",
    "load_phase1 = args.load_phase1\n",
    "\n",
    "assert (not (freeze_cls and freeze_exp))\n",
    "assert (not (train_exp_first and train_cls_first))\n",
    "assert not ((freeze_cls or freeze_exp) and (train_exp_first or train_cls_first))# can't freeze both in the same time\n",
    "\n",
    "LEARNING_RATE = 1e-5\n",
    "\n",
    "# static hyper-parameters\n",
    "MAX_SEQ_LENGTH = 512\n",
    "HARD_SELECTION_COUNT = None\n",
    "HARD_SELECTION_THRESHOLD = 0.5\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "# Set the output directory for saving model file\n",
    "# Optionally, set a GCP bucket location\n",
    "from losses import rnr_matrix_loss\n",
    "\n",
    "\n",
    "bert_size = 'base'\n",
    "#bert_size = 'large'\n",
    "\n",
    "if bert_size == 'base':\n",
    "    BERT_MODEL_HUB = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n",
    "\n",
    "#pooling = 'mean'\n",
    "pooling = 'first'\n",
    "\n",
    "EXP_OUTPUT = exp_structure\n",
    "\n",
    "rebalance_approach = 'resampling'\n",
    "#rebalance_approach = 'bayesian'\n",
    "if EXP_OUTPUT == 'gru':\n",
    "    if rebalance_approach == 'resampling':\n",
    "        loss_function = imbalanced_bce_resampling\n",
    "    else:\n",
    "        loss_function = imbalanced_bce_bayesian\n",
    "elif EXP_OUTPUT == 'rnr':\n",
    "    loss_function = rnr_matrix_loss\n",
    "    \n",
    "if freeze_cls:\n",
    "    suffix = 'freeze_cls'\n",
    "elif freeze_exp:\n",
    "    suffix = 'freeze_exp'\n",
    "elif train_cls_first:\n",
    "    suffix = 'train_cls_first'\n",
    "elif train_exp_first:\n",
    "    suffix = 'train_exp_first'\n",
    "else:\n",
    "    suffix = ''\n",
    "\n",
    "OUTPUT_DIR = ['bert_{}_seqlen_{}_{}_exp_output_{}'.format(bert_size, MAX_SEQ_LENGTH, dataset, EXP_OUTPUT)]\n",
    "OUTPUT_DIR.append('merged_evidences' if merge_evidences else 'separated_evidences')\n",
    "if train_on_portion != 0:\n",
    "    OUTPUT_DIR += ['train_on_portion', str(train_on_portion)]\n",
    "DATASET_CACHE_NAME = '_'.join(OUTPUT_DIR) + '_inputdata_cache'\n",
    "if par_lambda is None:\n",
    "    OUTPUT_DIR.append('no_weight')\n",
    "else:\n",
    "    OUTPUT_DIR.append('par_lambda_{}'.format(par_lambda))\n",
    "OUTPUT_DIR.append('no_padding_imbalanced_bce_{}_pooling_{}_learning_rate_{}'.format(rebalance_approach, pooling, LEARNING_RATE))\n",
    "OUTPUT_DIR.append(suffix)\n",
    "OUTPUT_DIR = '_'.join(OUTPUT_DIR)\n",
    "MODEL_NAME = OUTPUT_DIR\n",
    "OUTPUT_DIR = os.path.join('model_checkpoints', MODEL_NAME)\n",
    "\n",
    "# @markdown Whether or not to clear/delete the directory and create a new one\n",
    "#DO_DELETE = False  # @param {type:\"boolean\"}\n",
    "# @markdown Set USE_BUCKET and BUCKET if you want to (optionally) store model output on GCP bucket.\n",
    "USE_BUCKET = False  # @param {type:\"boolean\"}\n",
    "BUCKET = 'bert-base-uncased-test0'  # @param {type:\"string\"}\n",
    "\n",
    "if USE_BUCKET:\n",
    "    OUTPUT_DIR = 'gs://{}/{}'.format(BUCKET, OUTPUT_DIR)\n",
    "    from google.colab import auth\n",
    "    auth.authenticate_user()\n",
    "\n",
    "if DO_DELETE:\n",
    "    try:\n",
    "        tf.gfile.DeleteRecursively(OUTPUT_DIR)\n",
    "    except:\n",
    "        # Doesn't matter if the directory didn't exist\n",
    "        pass\n",
    "    \n",
    "def mkdirs(path):\n",
    "    if tensorflow.__version__.startswith('2'):\n",
    "        tf.io.gfile.makedirs(path)\n",
    "    else:\n",
    "        tf.gfile.MakeDirs(path)\n",
    "        \n",
    "mkdirs(OUTPUT_DIR)\n",
    "print('***** Model output directory: {} *****'.format(OUTPUT_DIR))\n",
    "\n",
    "# initializing graph and session\n",
    "graph = tf.get_default_graph()\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "config.gpu_options.visible_device_list = gpu_id\n",
    "config.graph_options.optimizer_options.global_jit_level = tf.OptimizerOptions.ON_1\n",
    "sess = tf.Session(config=config)\n",
    "set_session(sess)\n",
    "\n",
    "# data loading and preprocessing from eraser\n",
    "from eraserbenchmark.rationale_benchmark.utils import load_datasets, load_documents\n",
    "from eraserbenchmark.eraser_utils import extract_doc_ids_from_annotations\n",
    "from itertools import chain\n",
    "\n",
    "if dataset == 'movies':\n",
    "    label_list = ['POS', 'NEG']\n",
    "elif dataset == 'multirc':\n",
    "    label_list = ['True', 'False']\n",
    "elif dataset == 'fever':\n",
    "    label_list = ['SUPPORTS', 'REFUTES']\n",
    "\n",
    "data_dir = f'/home/zzhang/.keras/datasets/{dataset}/'\n",
    "train, val, test = load_datasets(data_dir)\n",
    "if train_on_portion != 0:\n",
    "    train = train[:int(len(train) * train_on_portion)]\n",
    "#print(train[-1])\n",
    "#train, val, test = [expand_on_evidences(data) for data in [train, val, test]]\n",
    "docids = set(chain.from_iterable(extract_doc_ids_from_annotations(d) for d in [train, val, test]))\n",
    "docs = load_documents(data_dir, docids)\n",
    "\n",
    "from bert_data_preprocessing_rational_eraser import preprocess\n",
    "@cache_decorator(os.path.join('cache', DATASET_CACHE_NAME + '_eraser_format'))\n",
    "def preprocess_wrapper(*data_inputs, docs=docs):\n",
    "    ret = []\n",
    "    for data in data_inputs:\n",
    "        ret.append(preprocess(data, docs, label_list, dataset, MAX_SEQ_LENGTH, EXP_OUTPUT, merge_evidences, gpu_id=gpu_id))\n",
    "    return ret\n",
    "\n",
    "rets_train, rets_val, rets_test = preprocess_wrapper(train, val, test, docs=docs)\n",
    "\n",
    "train_input_ids, train_input_masks, train_segment_ids, train_rations, train_labels = rets_train\n",
    "val_input_ids, val_input_masks, val_segment_ids, val_rations, val_labels = rets_val\n",
    "test_input_ids, test_input_masks, test_segment_ids, test_rations, test_labels = rets_test\n",
    "\n",
    "def expand_on_evidences(data):\n",
    "    from copy import deepcopy\n",
    "    from eraserbenchmark.rationale_benchmark.utils import Annotation\n",
    "    expanded_data = []\n",
    "    for ann in tqdm_notebook(data):\n",
    "        for ev_group in ann.evidences:\n",
    "            new_ann = Annotation(annotation_id=ann.annotation_id,\n",
    "                                 query=ann.query,\n",
    "                                 evidences=frozenset([ev_group]),\n",
    "                                 classification=ann.classification)\n",
    "            expanded_data.append(new_ann)\n",
    "    return expanded_data\n",
    "#expanded_test = expand_on_evidences(test)\n",
    "\n",
    "# hyper-parameters of BERT's\n",
    "WARMUP_PROPORTION = 0.1\n",
    "\n",
    "num_train_steps = int(len(train_input_ids) / BATCH_SIZE * float(NUM_EPOCHS))\n",
    "num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)\n",
    "\n",
    "from bert_utils import get_vocab\n",
    "vocab = get_vocab(config)\n",
    "\n",
    "# building models\n",
    "from model import BertLayer\n",
    "from tensorflow.keras.layers import CuDNNGRU, CuDNNLSTM\n",
    "from tensorflow.keras.layers import Bidirectional\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Reshape, Multiply, Concatenate, RepeatVector, Dot, Lambda, Add, Softmax\n",
    "\n",
    "from metrices import sp_precision_wrapper, sp_recall_wrapper\n",
    "\n",
    "DIM_DENSE_CLS = 256\n",
    "NUM_GRU_UNITS_BERT_SEQ = 128\n",
    "NUM_INTERVAL_LSTM_WIDTH = 100\n",
    "\n",
    "if par_lambda is None:\n",
    "    loss_weights = None\n",
    "else:\n",
    "    loss_weights = {'cls_output': 1,\n",
    "                    'exp_output': par_lambda}\n",
    "metrics = {'cls_output': 'accuracy',\n",
    "           'exp_output': [f1_wrapper(EXP_OUTPUT),\n",
    "                          sp_precision_wrapper(EXP_OUTPUT),\n",
    "                          sp_recall_wrapper(EXP_OUTPUT),\n",
    "                          precision_wrapper(EXP_OUTPUT),\n",
    "                          recall_wrapper(EXP_OUTPUT)]}\n",
    "loss = {'cls_output': 'binary_crossentropy',\n",
    "        'exp_output': loss_function()}\n",
    "\n",
    "def build_model(par_lambda=None):\n",
    "    in_id = Input(shape=(MAX_SEQ_LENGTH,), name=\"input_ids\")\n",
    "    in_mask = Input(shape=(MAX_SEQ_LENGTH,), name=\"input_masks\")\n",
    "    in_segment = Input(shape=(MAX_SEQ_LENGTH,), name=\"segment_ids\")\n",
    "    bert_inputs = [in_id, in_mask, in_segment]\n",
    "\n",
    "    bert_cls_output, bert_exp_output = BertLayer(\n",
    "        n_fine_tune_layers=10, name='bert')(bert_inputs)\n",
    "\n",
    "    outputs = []\n",
    "    if 'seq' not in dataset:\n",
    "        # Classifier output\n",
    "        dense = Dense(DIM_DENSE_CLS, activation='tanh', name='cls_dense')(bert_cls_output)\n",
    "        cls_output = Dense(1, activation='sigmoid', name='cls_output')(dense)\n",
    "        outputs.append(cls_output)\n",
    "    if 'cls' not in dataset:\n",
    "        # Explainer output\n",
    "        if EXP_OUTPUT == 'gru':\n",
    "            gru = CuDNNGRU(\n",
    "                NUM_GRU_UNITS_BERT_SEQ, kernel_initializer='random_uniform', return_sequences=True, name='exp_gru_gru')(bert_exp_output)\n",
    "            exp = Dense(1, activation='sigmoid', name='exp_gru_dense')(gru)\n",
    "            output_mask = Reshape((MAX_SEQ_LENGTH, 1), name='exp_gru_reshape')(in_mask)\n",
    "            exp_outputs = Multiply(name='exp_output')([output_mask, exp])\n",
    "        elif EXP_OUTPUT == 'rnr':\n",
    "            M1 = Bidirectional(layer=CuDNNLSTM(NUM_INTERVAL_LSTM_WIDTH, return_sequences=True, name='exp_rnr_lstm1'),\n",
    "                               merge_mode='concat', name='exp_rnr_bidirectional1')(bert_exp_output)\n",
    "            p_starts = Dense(1, activation='sigmoid', name='exp_rnr_starts')(Concatenate(axis=-1)([bert_exp_output, M1]))\n",
    "            start_mask = Reshape((MAX_SEQ_LENGTH, 1))(in_mask)\n",
    "            p_starts = Multiply()([p_starts, start_mask])\n",
    "            \n",
    "            m1_tilde = Dot(axes=-2)([p_starts, M1])\n",
    "            M1_tilde = Lambda(lambda x: tf.tile(x, (1, MAX_SEQ_LENGTH, 1)))(m1_tilde)\n",
    "            x = Multiply()([M1, M1_tilde])\n",
    "            M2 = Bidirectional(layer=CuDNNLSTM(NUM_INTERVAL_LSTM_WIDTH, return_sequences=True, name='exp_rnr_lstm2'),\n",
    "                               merge_mode='concat', name='exp_rnr_bidirecitonal2')(Concatenate(axis=-1)([bert_exp_output, M1, M1_tilde, x]))\n",
    "            p_end_given_start = Dense(MAX_SEQ_LENGTH, activation='linear', name='exp_rnr_end')(Concatenate(axis=-1)([bert_exp_output, M2]))\n",
    "            end_mask = Lambda(lambda x: tf.tile(x, (1, MAX_SEQ_LENGTH, 1)))(Reshape((1, MAX_SEQ_LENGTH))(in_mask))\n",
    "            p_end_given_start = Multiply()([p_end_given_start, end_mask])\n",
    "            p_end_given_start = Lambda(lambda x: tf.linalg.band_part(x, 0, -1))(p_end_given_start)\n",
    "            p_end_given_start = Softmax(axis=-1)(p_end_given_start)\n",
    "            \n",
    "            exp_outputs = Concatenate(axis=-1, name='exp_output')([p_starts, p_end_given_start])\n",
    "            #exp_outputs = Lambda(lambda x: tf.reduce_sum(x, axis=-1, keepdims=True), name='exp_output')(p_dist)\n",
    "        outputs.append(exp_outputs)\n",
    "\n",
    "    model = Model(inputs=bert_inputs, outputs=outputs)\n",
    "    optimizer = Adam(LEARNING_RATE)\n",
    "    \n",
    "    if par_lambda is None:\n",
    "        loss_weights = None\n",
    "    else:\n",
    "        loss_weights = {'cls_output': 1,\n",
    "                        'exp_output': par_lambda}\n",
    "    metrics = {'cls_output': 'accuracy',\n",
    "               'exp_output': [f1_wrapper(EXP_OUTPUT),\n",
    "                              sp_precision_wrapper(EXP_OUTPUT),\n",
    "                              sp_recall_wrapper(EXP_OUTPUT),\n",
    "                              precision_wrapper(EXP_OUTPUT),\n",
    "                              recall_wrapper(EXP_OUTPUT)]}\n",
    "    loss = {'cls_output': 'binary_crossentropy',\n",
    "            'exp_output': loss_function()}\n",
    "    '''\n",
    "    metrics = [f1_wrapper(EXP_OUTPUT),\n",
    "                              sp_precision_wrapper(EXP_OUTPUT),\n",
    "                              sp_recall_wrapper(EXP_OUTPUT),\n",
    "                              precision_wrapper(EXP_OUTPUT),\n",
    "                              recall_wrapper(EXP_OUTPUT)]\n",
    "    loss = loss_function()\n",
    "    '''\n",
    "    model.compile(loss=loss, loss_weights=loss_weights, optimizer=optimizer, metrics=metrics)\n",
    "\n",
    "    model_exp = Model(inputs=bert_inputs, outputs=exp_outputs)\n",
    "    optimizer = Adam(LEARNING_RATE)\n",
    "    model_exp.compile(loss=loss['exp_output'], optimizer=optimizer, metrics=[metrics['exp_output']])\n",
    "    \n",
    "    model_cls = Model(inputs=bert_inputs, outputs=cls_output)\n",
    "    optimizer = Adam(LEARNING_RATE)\n",
    "    model_cls.compile(loss=loss['cls_output'], optimizer=optimizer, metrics=[metrics['cls_output']])\n",
    "\n",
    "    return model, model_cls, model_exp\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "# training, evaluation and inference\n",
    "import os\n",
    "SAVE_CHECKPOINTS_STEPS = 500\n",
    "SAVE_SUMMARY_STEPS = 100\n",
    "\n",
    "checkpoint_path = os.path.join(OUTPUT_DIR, 'cp-{epoch:04d}.ckpt')\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "cls_output_file = os.path.join(OUTPUT_DIR, 'output.txt')\n",
    "\n",
    "RES_FOR_BENCHMARK_FNAME = MODEL_NAME + '_' + BENCHMARK_SPLIT_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": [
     44,
     208,
     240
    ],
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method BertLayer.call of <model.BertLayer object at 0x7f52d59bdc50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BertLayer.call of <model.BertLayer object at 0x7f52d59bdc50>>: AttributeError: module 'gast' has no attribute 'Num'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method BertLayer.call of <model.BertLayer object at 0x7f52d59bdc50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BertLayer.call of <model.BertLayer object at 0x7f52d59bdc50>>: AttributeError: module 'gast' has no attribute 'Num'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method BertLayer.call of <model.BertLayer object at 0x7f52d59bdc50>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BertLayer.call of <model.BertLayer object at 0x7f52d59bdc50>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_9\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_masks (InputLayer)        [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bert (BertLayer)                [(None, 768), (None, 110104890   input_ids[0][0]                  \n",
      "                                                                 input_masks[0][0]                \n",
      "                                                                 segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "exp_rnr_bidirectional1 (Bidirec (None, 512, 200)     696000      bert[0][1]                       \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 512, 968)     0           bert[0][1]                       \n",
      "                                                                 exp_rnr_bidirectional1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "exp_rnr_starts (Dense)          (None, 512, 1)       969         concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "reshape_6 (Reshape)             (None, 512, 1)       0           input_masks[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "multiply_9 (Multiply)           (None, 512, 1)       0           exp_rnr_starts[0][0]             \n",
      "                                                                 reshape_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dot_3 (Dot)                     (None, 1, 200)       0           multiply_9[0][0]                 \n",
      "                                                                 exp_rnr_bidirectional1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_9 (Lambda)               (None, 512, 200)     0           dot_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "multiply_10 (Multiply)          (None, 512, 200)     0           exp_rnr_bidirectional1[0][0]     \n",
      "                                                                 lambda_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_10 (Concatenate)    (None, 512, 1368)    0           bert[0][1]                       \n",
      "                                                                 exp_rnr_bidirectional1[0][0]     \n",
      "                                                                 lambda_9[0][0]                   \n",
      "                                                                 multiply_10[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "exp_rnr_bidirecitonal2 (Bidirec (None, 512, 200)     1176000     concatenate_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_11 (Concatenate)    (None, 512, 968)     0           bert[0][1]                       \n",
      "                                                                 exp_rnr_bidirecitonal2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "reshape_7 (Reshape)             (None, 1, 512)       0           input_masks[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "exp_rnr_end (Dense)             (None, 512, 512)     496128      concatenate_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "lambda_10 (Lambda)              (None, 512, 512)     0           reshape_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "multiply_11 (Multiply)          (None, 512, 512)     0           exp_rnr_end[0][0]                \n",
      "                                                                 lambda_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_11 (Lambda)              (None, 512, 512)     0           multiply_11[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "cls_dense (Dense)               (None, 256)          196864      bert[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "softmax_3 (Softmax)             (None, 512, 512)     0           lambda_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "cls_output (Dense)              (None, 1)            257         cls_dense[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "exp_output (Concatenate)        (None, 512, 513)     0           multiply_9[0][0]                 \n",
      "                                                                 softmax_3[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 112,671,108\n",
      "Trainable params: 73,444,938\n",
      "Non-trainable params: 39,226,170\n",
      "__________________________________________________________________________________________________\n",
      "marked rationals are saved under model_checkpoints/bert_base_seqlen_512_fever_exp_output_rnr_merged_evidences_par_lambda_0.2_no_padding_imbalanced_bce_resampling_pooling_first_learning_rate_1e-05_/exp_outputs/\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bff3465b90714f17a273302ec0812076",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6111), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from eraser_benchmark import rnr_matrix_to_rational_mask\n",
    "\n",
    "with graph.as_default():\n",
    "    set_session(sess)\n",
    "    model, model_cls, model_exp = build_model(par_lambda)\n",
    "    model.summary()\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.tables_initializer())\n",
    "\n",
    "    training_inputs = [train_input_ids, train_input_masks, train_segment_ids]\n",
    "    # training_inputs = [train_input_ids[:10],\n",
    "    #                   train_input_masks[:10], train_segment_ids[:10]]\n",
    "    val_inputs = [val_input_ids, val_input_masks, val_segment_ids]\n",
    "    test_inputs = [test_input_ids, test_input_masks, test_segment_ids]\n",
    "\n",
    "    training_outputs, test_outputs, val_outputs = {}, {}, {}\n",
    "\n",
    "    if 'seq' not in dataset:\n",
    "        training_outputs['cls_output'] = train_labels\n",
    "        #training_outputs['cls_output'] = train_labels[:10]\n",
    "        test_outputs['cls_output'] = test_labels\n",
    "        val_outputs['cls_output'] = val_labels\n",
    "    if 'cls' not in dataset:\n",
    "        training_outputs['exp_output'] = train_rations\n",
    "        #training_outputs['exp_output'] = train_rations[:10]\n",
    "        test_outputs['exp_output'] = test_rations\n",
    "        val_outputs['exp_output'] = val_rations\n",
    "\n",
    "    initial_epoch = 0\n",
    "    if load_best:\n",
    "        with open(cls_output_file, 'r') as fin:\n",
    "            log = fin.readlines()\n",
    "        history = eval(log[2])\n",
    "        best_epoch = np.argmin(history['loss'])+1\n",
    "        model.load_weights(checkpoint_path.format(epoch=best_epoch))\n",
    "    else:\n",
    "        for ckpt_i in range(NUM_EPOCHS, 0, -1):\n",
    "            if os.path.isfile(checkpoint_path.format(epoch=ckpt_i)):\n",
    "                initial_epoch = ckpt_i\n",
    "                model.load_weights(checkpoint_path.format(epoch=ckpt_i))\n",
    "                # assert False  # dumm proof, most of the case the training is end-to-end, without disturbance and reloading\n",
    "                break\n",
    "\n",
    "    if do_train:\n",
    "        cp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path,\n",
    "                                                         save_weights_only=False,\n",
    "                                                         verbose=1,\n",
    "                                                         period=1)\n",
    "        es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "        with open(cls_output_file, 'a+') as fw:\n",
    "            fw.write(\"=============== {} ===============\\n\".format(datetime.now()))\n",
    "        if train_cls_first or train_exp_first:\n",
    "            if train_cls_first:\n",
    "                phases = ['cls', 'exp']\n",
    "                model_phase0 = model_cls\n",
    "                model_phase1 = model_exp\n",
    "                training_outputs_phase0, val_outputs_phase0 = training_outputs['cls_output'], val_outputs['cls_output']\n",
    "                training_outputs_phase1, val_outputs_phase1 = training_outputs['exp_output'], val_outputs['exp_output']\n",
    "            elif train_exp_first:\n",
    "                phases = ['exp', 'cls']\n",
    "                model_phase0 = model_exp\n",
    "                model_phase1 = model_cls\n",
    "                training_outputs_phase0, val_outputs_phase0 = training_outputs['exp_output'], val_outputs['exp_output']\n",
    "                training_outputs_phase1, val_outputs_phase1 = training_outputs['cls_output'], val_outputs['cls_output']\n",
    "            for layer in model.layers:\n",
    "                if layer.name.startswith(phases[1]):\n",
    "                    layer.trainable = False\n",
    "            checkpoint_path_phase0 = os.path.join(OUTPUT_DIR, 'phase0', 'cp-{epoch:04d}.ckpt')\n",
    "            mkdirs(os.path.join(OUTPUT_DIR, 'phase0'))\n",
    "            cp_callback_phase0 = tf.keras.callbacks.ModelCheckpoint(checkpoint_path_phase0,\n",
    "                                                                     save_weights_only=False,\n",
    "                                                                     verbose=1,\n",
    "                                                                     period=1)\n",
    "            if start_from_phase1:\n",
    "                with open(cls_output_file, 'r') as fin:\n",
    "                    log = fin.readlines()\n",
    "                history = eval(log[2])\n",
    "                best_epoch = np.argmin(history['loss'])+1\n",
    "                model_phase0.load_weights(checkpoint_path_phase0.format(epoch=best_epoch))\n",
    "            else:\n",
    "                history = model_phase0.fit(\n",
    "                    training_inputs,\n",
    "                    training_outputs_phase0,\n",
    "                    validation_data=(val_inputs, val_outputs_phase0),\n",
    "                    epochs=NUM_EPOCHS,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    callbacks=[cp_callback_phase0, es_callback], \n",
    "                    initial_epoch=initial_epoch\n",
    "                )\n",
    "                with open(cls_output_file, 'a+') as fw:\n",
    "                    fw.write(\"phase 0 training history {}:\\n\".format(datetime.now()))\n",
    "                    fw.write(str(history.history) + '\\n')\n",
    "                evaluation_res = model.evaluate(x=test_inputs,\n",
    "                                                y=test_outputs,\n",
    "                                                batch_size=BATCH_SIZE,\n",
    "                                                verbose=1)\n",
    "                with open(cls_output_file, 'a+') as fw:\n",
    "                    fw.write(\"phase 0 evaluation {}:\\n\".format(datetime.now()))\n",
    "                    fw.write(str(evaluation_res) + '\\n')\n",
    "            for layer in model.layers:\n",
    "                if layer.name == 'bert' or layer.name.startswith(phases[0]):\n",
    "                    layer.trainable = False\n",
    "                elif layer.name.startswith(phases[1]):\n",
    "                    layer.trainable = True\n",
    "            for layer in model_phase1.layers:\n",
    "                if layer.name == 'bert' or layer.name.startswith(phases[0]):\n",
    "                    layer.trainable = False\n",
    "                elif layer.name.startswith(phases[1]):\n",
    "                    layer.trainable = True\n",
    "            optimizer = Adam(LEARNING_RATE)\n",
    "            model_phase1.compile(loss=loss[phases[1]+'_output'], optimizer=optimizer, metrics=[metrics[phases[1]+'_output']])\n",
    "            print(f'trainable variables: {[v.name for v in model_phase1.trainable_variables]}')\n",
    "            checkpoint_path_phase1 = os.path.join(OUTPUT_DIR, 'phase1', 'cp-{epoch:04d}.ckpt')\n",
    "            mkdirs(os.path.join(OUTPUT_DIR, 'phase1'))\n",
    "            cp_callback_phase1 = tf.keras.callbacks.ModelCheckpoint(checkpoint_path_phase1,\n",
    "                                                                     save_weights_only=False,\n",
    "                                                                     verbose=1,\n",
    "                                                                     period=1)\n",
    "            if load_phase1:\n",
    "                for ckpt_i in range(NUM_EPOCHS, 0, -1):\n",
    "                    if os.path.isfile(checkpoint_path_phase1.format(epoch=ckpt_i)):\n",
    "                        initial_epoch = ckpt_i\n",
    "                        model_phase1.load_weights(checkpoint_path_phase1.format(epoch=ckpt_i))\n",
    "                        # assert False  # dumm proof, most of the case the training is end-to-end, without disturbance and reloading\n",
    "                        break\n",
    "                with open(cls_output_file, 'a+') as fw:\n",
    "                    fw.write(\"phase 1 loaded, no training history {}:\\n\".format(datetime.now()))\n",
    "            else:\n",
    "                history = model_phase1.fit(\n",
    "                    training_inputs,\n",
    "                    training_outputs_phase1,\n",
    "                    validation_data=(val_inputs, val_outputs_phase1),\n",
    "                    epochs=NUM_EPOCHS,\n",
    "                    initial_epoch=initial_epoch,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    callbacks=[cp_callback_phase1, es_callback],\n",
    "                )\n",
    "                with open(cls_output_file, 'a+') as fw:\n",
    "                    fw.write(\"phase 1 training history {}:\\n\".format(datetime.now()))\n",
    "                    fw.write(str(history.history) + '\\n')\n",
    "            evaluation_res = model.evaluate(x=test_inputs,\n",
    "                                            y=test_outputs,\n",
    "                                            batch_size=BATCH_SIZE,\n",
    "                                            verbose=1)\n",
    "            with open(cls_output_file, 'a+') as fw:\n",
    "                fw.write(\"phase 1 evaluation {}:\\n\".format(datetime.now()))\n",
    "                fw.write(str(evaluation_res) + '\\n')\n",
    "                \n",
    "        elif freeze_cls or freeze_exp:\n",
    "            if freeze_cls:\n",
    "                phases = ['cls', 'exp']\n",
    "            elif freeze_exp:\n",
    "                phases = ['exp', 'cls']\n",
    "            for layer in model.layers:\n",
    "                if layer.name.startswith(phases[1]):\n",
    "                    layer.trainable = False\n",
    "            print(f'trainable variables: {[v.name for v in model.trainable_variables]}')\n",
    "            history = model.fit(\n",
    "                training_inputs,\n",
    "                training_outputs,\n",
    "\n",
    "                validation_data=(val_inputs,\n",
    "                                 val_outputs),\n",
    "                epochs=NUM_EPOCHS,\n",
    "                batch_size=BATCH_SIZE,\n",
    "                callbacks=[cp_callback, es_callback],\n",
    "                initial_epoch=initial_epoch\n",
    "            )\n",
    "            with open(cls_output_file, 'a+') as fw:\n",
    "                fw.write(\"{}:\\n\".format(datetime.now()))\n",
    "                fw.write(str(history.history) + '\\n')\n",
    "            for layer in model.layers:\n",
    "                if layer.name == 'bert' or layer.name.startswith(phases[0]):\n",
    "                    layer.trainable = False\n",
    "                elif layer.name.startswith(phases[1]):\n",
    "                    layer.trainable = True\n",
    "            print(f'trainable variables: {[v.name for v in model.trainable_variables]}')\n",
    "            history = model.fit(\n",
    "                training_inputs,\n",
    "                training_outputs,\n",
    "\n",
    "                validation_data=(val_inputs,\n",
    "                                 val_outputs),\n",
    "                epochs=NUM_EPOCHS,\n",
    "                batch_size=BATCH_SIZE,\n",
    "                callbacks=[cp_callback, es_callback],\n",
    "            )\n",
    "            with open(cls_output_file, 'a+') as fw:\n",
    "                fw.write(\"{}:\\n\".format(datetime.now()))\n",
    "                fw.write(str(history.history) + '\\n')\n",
    "        else:\n",
    "            history = model.fit(\n",
    "                training_inputs,\n",
    "                training_outputs,\n",
    "\n",
    "                validation_data=(val_inputs,\n",
    "                                 val_outputs),\n",
    "                epochs=NUM_EPOCHS,\n",
    "                batch_size=BATCH_SIZE,\n",
    "                callbacks=[cp_callback, es_callback],\n",
    "                initial_epoch=initial_epoch\n",
    "            )\n",
    "            with open(cls_output_file, 'a+') as fw:\n",
    "                fw.write(\"{}:\\n\".format(datetime.now()))\n",
    "                fw.write(str(history.history) + '\\n')     \n",
    "\n",
    "    if evaluate:\n",
    "        evaluation_res = model.evaluate(x=test_inputs,\n",
    "                                        y=test_outputs,\n",
    "                                        batch_size=BATCH_SIZE,\n",
    "                                        verbose=1)\n",
    "        with open(cls_output_file, 'a+') as fw:\n",
    "            fw.write(\"{}:\\n\".format(datetime.now()))\n",
    "            fw.write(str(evaluation_res) + '\\n')\n",
    "\n",
    "    if exp_visualize:\n",
    "        len_head = 100\n",
    "        test_inputs_head = [x[:len_head] for x in test_inputs]\n",
    "        pred = model_exp.predict(test_inputs_head)\n",
    "        pred = np.round(np.array(pred)).astype(np.int32)\n",
    "        exp_output_folder = os.path.join(OUTPUT_DIR, 'exp_outputs/')\n",
    "        mkdirs(exp_output_folder)\n",
    "        print('marked rationals are saved under {}'.format(exp_output_folder))\n",
    "        for i, l in enumerate(tqdm_notebook(test)):\n",
    "            if i == len_head:\n",
    "                break\n",
    "            input_ids = test_input_ids[i]\n",
    "            if EXP_OUTPUT == 'rnr':\n",
    "                pred_intp = rnr_matrix_to_rational_mask(pred[i])[0]\n",
    "            elif EXP_OUTPUT == 'gru':\n",
    "                pred_intp = pred[i].reshape([-1])\n",
    "            label = test_labels[i]\n",
    "            gt = test_rations[i].reshape([-1])\n",
    "            html = convert_res_to_htmls(input_ids, pred_intp, gt, vocab)\n",
    "            with open(exp_output_folder + l.annotation_id+'-'+l.docids[0] + '.html', \"w+\") as f:\n",
    "                f.write('<h1>label: {}</h1>\\n'.format('pos' if label == 1 else 'neg'))\n",
    "                f.write(html[1] + '<br/><br/>\\n' + html[0])\n",
    "\n",
    "    if exp_benchmark:\n",
    "        from eraser_benchmark import pred_to_results\n",
    "        result_fname = RES_FOR_BENCHMARK_FNAME + '.jsonl'\n",
    "        result_fname = os.path.join(\n",
    "            'eraserbenchmark', 'annotated_by_exp', result_fname)\n",
    "\n",
    "        if BENCHMARK_SPLIT_NAME == 'test':\n",
    "            benchmark_inputs, raw_input, benchmark_input_ids = test_inputs, test, test_input_ids\n",
    "        elif BENCHMARK_SPLIT_NAME == 'val':\n",
    "            benchmark_inputs, raw_input, benchmark_input_ids = val_inputs, val, val_input_ids\n",
    "        elif BENCHMARK_SPLIT_NAME == 'train':\n",
    "            benchmark_inputs, raw_input, benchmark_input_ids = training_inputs, train, train_input_ids\n",
    "\n",
    "        pred = model.predict(x=benchmark_inputs)\n",
    "\n",
    "        # results = [pred_to_results(raw_input.loc[i], benchmark_input_ids[i], (pred[0][i], pred[1][i]), HARD_SELECTION_COUNT, HARD_SELECTION_THRESHOLD)\n",
    "        #           for i in range(len(raw_input))]\n",
    "        from eraser_benchmark import pred_to_results, get_cls_score, add_cls_scores, remove_rations, extract_rations\n",
    "        results = [pred_to_results(raw_input[i], benchmark_input_ids[i], \n",
    "                                   (pred[0][i], pred[1][i]), \n",
    "                                   HARD_SELECTION_COUNT, \n",
    "                                   HARD_SELECTION_THRESHOLD,\n",
    "                                   vocab, docs, label_list, EXP_OUTPUT)\n",
    "                   for i in range(len(pred[0]))]\n",
    "        pred_softmax = np.hstack([1-pred[0], pred[0]])\n",
    "        c_pred_softmax = get_cls_score(\n",
    "            model, results, docs, label_list, dataset, remove_rations, MAX_SEQ_LENGTH, EXP_OUTPUT, gpu_id=gpu_id)\n",
    "        s_pred_softmax = get_cls_score(\n",
    "            model, results, docs, label_list, dataset, extract_rations, MAX_SEQ_LENGTH, EXP_OUTPUT, gpu_id=gpu_id)\n",
    "\n",
    "        results = [add_cls_scores(res, cls_score, c_cls_score, s_cls_score, label_list) for res, cls_score,\n",
    "                   c_cls_score, s_cls_score in zip(results, pred_softmax, c_pred_softmax, s_pred_softmax)]\n",
    "        anns_saved = set()\n",
    "        real_results = []\n",
    "        for ann in results:\n",
    "            if ann['annotation_id'] not in anns_saved:\n",
    "                anns_saved.add(ann['annotation_id'])\n",
    "                real_results.append(ann)\n",
    "        with open(result_fname+'pkl3', \"wb+\") as pfout:\n",
    "            pickle.dump(real_results, pfout)\n",
    "        from eraserbenchmark.eraser import evaluate\n",
    "        evaluate(MODEL_NAME, dataset, BENCHMARK_SPLIT_NAME, train_on_portion)\n",
    "\n",
    "    with open(cls_output_file, 'a+') as fw:\n",
    "        fw.write('/////////////////experiment ends//////////////////\\n\\n\\n')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "“Predicting Movie Reviews with BERT on TF Hub.ipynb”的副本",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "493.667px",
    "left": "900px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
