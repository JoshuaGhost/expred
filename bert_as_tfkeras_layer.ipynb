{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 183
    },
    "colab_type": "code",
    "id": "hsZvic2YxnTz",
    "outputId": "35722f7e-f064-4fb8-ecf7-c9de0b745a22"
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "from utils import *\n",
    "from display_rational import convert_res_to_htmls\n",
    "from config import *\n",
    "from losses import imbalanced_bce_bayesian, imbalanced_bce_resampling, exp_interval_loss\n",
    "from metrices import *\n",
    "from tqdm import tqdm_notebook\n",
    "from bert import optimization\n",
    "from bert import run_classifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.python.keras.models import load_model\n",
    "from tensorflow.python.keras.backend import set_session\n",
    "from tensorflow.keras import backend as K\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow_hub as hub\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import bert\n",
    "\n",
    "#import tensorflow.compat.v1 as tf\n",
    "# tf.disable_v2_behavior()\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss of the multi-task learning is:\n",
    "$$\n",
    "\\mathcal{L}_{total} = \\mathcal{L}_{cls} + \\lambda\\mathcal{L}_{exp}\\text{,}\n",
    "$$\n",
    "where $\\mathcal{L}_{cls}$ is the loss of the classification task, $\\mathcal{L}_{exp}$ is the token-wise average of explaination loss (averaged cross entropy) and $S$ indicates the length of the input text. The hyper-parameter $\\lambda$ is written as ```par_lambda``` in the following cells since 'lambda' is a reserved word in python.\n",
    "\n",
    "The loss function has been changed since 2020.01.08. The old formulation of the loss function tried to balance between cls loss and exp loss, which down-scale both term as well as their learning rate. The new schema respect the global learning rate w.r.t. cls part while one can modify the learning rate of explaination part through $\\lambda$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# hyper-parameters\n",
    "MAX_SEQ_LENGTH = 512\n",
    "HARD_SELECTION_COUNT = None\n",
    "HARD_SELECTION_THRESHOLD = 0.5\n",
    "\n",
    "BATCH_SIZE = 2   # for jupyter-notebook\n",
    "# BATCH_SIZE = 16  # for stand-alone python script\n",
    "\n",
    "#par_lambda = None\n",
    "#par_lambda = 1e-5\n",
    "par_lambda = 1\n",
    "\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "LEARNING_RATE = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "both",
    "code_folding": [
     0
    ],
    "colab": {},
    "colab_type": "code",
    "id": "US_EAnICvP7f"
   },
   "outputs": [],
   "source": [
    "# Set the output directory for saving model file\n",
    "# Optionally, set a GCP bucket location\n",
    "\n",
    "dataset = 'eraser_movie_mtl'\n",
    "#dataset = 'eraser_multirc'\n",
    "#dataset = 'eraser_fever'\n",
    "\n",
    "rebalance_approach = 'resampling'\n",
    "#rebalance_approach = 'bayesian'\n",
    "\n",
    "bert_size = 'base'\n",
    "#bert_size = 'large'\n",
    "\n",
    "if bert_size == 'base':\n",
    "    BERT_MODEL_HUB = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n",
    "\n",
    "#pooling = 'mean'\n",
    "pooling = 'first'\n",
    "\n",
    "if rebalance_approach == 'resampling':\n",
    "    loss_function = imbalanced_bce_resampling\n",
    "else:\n",
    "    loss_function = imbalanced_bce_bayesian\n",
    "\n",
    "EXP_OUTPUT = 'gru'\n",
    "#EXP_OUTPUT = 'interval'\n",
    "\n",
    "suffix = ''\n",
    "#suffix = 'cls_only'\n",
    "#suffix = 'transfer_cls_to_exp'\n",
    "#suffix = 'transfer_exp_to_cls'\n",
    "\n",
    "OUTPUT_DIR = ['bert_{}_seqlen_{}_{}_exp_output_{}'.format(\n",
    "    bert_size, MAX_SEQ_LENGTH, dataset, EXP_OUTPUT)]\n",
    "DATASET_NAME = '_'.join(OUTPUT_DIR) + '_inputdata_cache'\n",
    "if par_lambda is None:\n",
    "    OUTPUT_DIR.append('no_weight')\n",
    "else:\n",
    "    OUTPUT_DIR.append('par_lambda_{}'.format(par_lambda))\n",
    "OUTPUT_DIR.append('no_padding_imbalanced_bce_{}_pooling_{}_learning_rate_{}'.format(\n",
    "    rebalance_approach, pooling, LEARNING_RATE))\n",
    "OUTPUT_DIR.append(suffix)\n",
    "\n",
    "OUTPUT_DIR = '_'.join(OUTPUT_DIR)\n",
    "MODEL_NAME = OUTPUT_DIR\n",
    "OUTPUT_DIR = os.path.join('model_checkpoints', MODEL_NAME)\n",
    "\n",
    "# @markdown Whether or not to clear/delete the directory and create a new one\n",
    "DO_DELETE = False  # @param {type:\"boolean\"}\n",
    "# @markdown Set USE_BUCKET and BUCKET if you want to (optionally) store model output on GCP bucket.\n",
    "USE_BUCKET = False  # @param {type:\"boolean\"}\n",
    "BUCKET = 'bert-base-uncased-test0'  # @param {type:\"string\"}\n",
    "\n",
    "if USE_BUCKET:\n",
    "    OUTPUT_DIR = 'gs://{}/{}'.format(BUCKET, OUTPUT_DIR)\n",
    "    from google.colab import auth\n",
    "    auth.authenticate_user()\n",
    "\n",
    "if DO_DELETE:\n",
    "    try:\n",
    "        tf.gfile.DeleteRecursively(OUTPUT_DIR)\n",
    "    except:\n",
    "        # Doesn't matter if the directory didn't exist\n",
    "        pass\n",
    "tf.io.gfile.makedirs(OUTPUT_DIR)\n",
    "print('***** Model output directory: {} *****'.format(OUTPUT_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# data loading\n",
    "if dataset == 'semeval18':\n",
    "    from load_data_semeval18 import download_and_load_datasets\n",
    "    DATA_COLUMNS = 'Tweet text'\n",
    "    LABEL_COLUMN = 'Label'\n",
    "elif dataset == 'eraser_multirc':\n",
    "    from load_data_eraser_multirc import download_and_load_datasets\n",
    "    DATA_COLUMNS = ['query', 'passage']\n",
    "    LABEL_COLUMN = 'classification'\n",
    "elif dataset == 'eraser_fever':\n",
    "    from load_data_eraser_fever import download_and_load_datasets\n",
    "    DATA_COLUMNS = ['query', 'passage']\n",
    "    LABEL_COLUMN = 'classification'\n",
    "else:\n",
    "    if dataset == 'acl_imdb' or dataset == 'acl_imdb_cls':\n",
    "        from load_data_acl_imdb import download_and_load_datasets\n",
    "    if dataset == 'semeval16' or dataset == 'semeval16_cls':\n",
    "        from load_data_semeval16 import download_and_load_datasets\n",
    "    if dataset == 'zaidan07_cls':\n",
    "        from load_data_imdb_zaidan07_cls import download_and_load_datasets\n",
    "    if dataset == 'zaidan07_seq' or dataset == 'zaidan07_mtl':\n",
    "        from load_data_imdb_zaidan07_seq import download_and_load_datasets\n",
    "    if dataset == 'eraser_movie_mtl':\n",
    "        from load_data_imdb_zaidan07_eraser import download_and_load_datasets\n",
    "    DATA_COLUMNS = ['sentence']\n",
    "    LABEL_COLUMN = 'polarity'\n",
    "\n",
    "ret = download_and_load_datasets()\n",
    "if len(ret) == 3:\n",
    "    train, val, test = ret\n",
    "else:\n",
    "    train, test = ret\n",
    "    val = train.sample(frac=0.2)\n",
    "    train = pd.merge(train, val, how='outer', indicator=True)\n",
    "    train = train.loc[train._merge == 'left_only', ['sentence', 'polarity']]\n",
    "label_list = [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# data preprocessing\n",
    "from bert_data_preprocessing_rational import load_bert_features, convert_bert_features\n",
    "\n",
    "\n",
    "def preprocess(data, label_list, dataset_name):\n",
    "    features = load_bert_features(\n",
    "        data, label_list, MAX_SEQ_LENGTH, DATA_COLUMNS, LABEL_COLUMN)\n",
    "\n",
    "    with_rations = ('cls' not in dataset_name)\n",
    "    with_lable_id = ('seq' not in dataset_name)\n",
    "\n",
    "    return convert_bert_features(features, with_lable_id, with_rations, EXP_OUTPUT)\n",
    "\n",
    "\n",
    "@cache_decorator(os.path.join('cache', DATASET_NAME))\n",
    "def preprocess_wrapper(*data_inputs):\n",
    "    ret = []\n",
    "    for data in data_inputs:\n",
    "        ret.append(preprocess(data, label_list, dataset))\n",
    "    return ret\n",
    "\n",
    "\n",
    "rets_train, rets_val, rets_test = preprocess_wrapper(train, val, test)\n",
    "\n",
    "train_input_ids, train_input_masks, train_segment_ids, train_rations, train_labels = rets_train\n",
    "val_input_ids, val_input_masks, val_segment_ids, val_rations, val_labels = rets_val\n",
    "test_input_ids, test_input_masks, test_segment_ids, test_rations, test_labels = rets_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# initializing graph and session\n",
    "graph = tf.get_default_graph()\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "config.gpu_options.visible_device_list = '0'\n",
    "config.graph_options.optimizer_options.global_jit_level = tf.OptimizerOptions.ON_1\n",
    "sess = tf.Session(config=config)\n",
    "set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# hyper-parameters of BERT's\n",
    "WARMUP_PROPORTION = 0.1\n",
    "\n",
    "num_train_steps = int(len(train_input_ids) / BATCH_SIZE * float(NUM_EPOCHS))\n",
    "num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# building models\n",
    "from model import BertLayer\n",
    "from tensorflow.keras.layers import CuDNNGRU, CuDNNLSTM\n",
    "from tensorflow.keras.layers import Bidirectional\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Reshape, Multiply, Concatenate, RepeatVector, Dot, Lambda\n",
    "\n",
    "DIM_DENSE_CLS = 256\n",
    "NUM_GRU_UNITS_BERT_SEQ = 128\n",
    "NUM_INTERVAL_LSTM_WIDTH = 100\n",
    "\n",
    "\n",
    "def build_model(par_lambda=None):\n",
    "    in_id = Input(shape=(MAX_SEQ_LENGTH,), name=\"input_ids\")\n",
    "    in_mask = Input(shape=(MAX_SEQ_LENGTH,), name=\"input_masks\")\n",
    "    in_segment = Input(shape=(MAX_SEQ_LENGTH,), name=\"segment_ids\")\n",
    "    bert_inputs = [in_id, in_mask, in_segment]\n",
    "\n",
    "    bert_cls_output, bert_exp_output = BertLayer(\n",
    "        n_fine_tune_layers=10)(bert_inputs)\n",
    "\n",
    "    outputs = []\n",
    "    if 'seq' not in dataset:\n",
    "        # Classifier output\n",
    "        dense = Dense(DIM_DENSE_CLS, activation='tanh')(bert_cls_output)\n",
    "        cls = Dense(1, activation='sigmoid', name='cls_output')(dense)\n",
    "        outputs.append(cls)\n",
    "    if 'cls' not in dataset:\n",
    "        # Explainer output\n",
    "        if EXP_OUTPUT == 'gru':\n",
    "            gru = CuDNNGRU(\n",
    "                NUM_GRU_UNITS_BERT_SEQ, kernel_initializer='random_uniform', return_sequences=True)(bert_exp_output)\n",
    "            exp = Dense(1, activation='sigmoid')(gru)\n",
    "            output_mask = Reshape((512, 1))(in_mask)\n",
    "            exp_outputs = Multiply(name='exp_output')([output_mask, exp])\n",
    "        elif EXP_OUTPUT == 'interval':\n",
    "            M1 = Bidirectional(layer=CuDNNLSTM(NUM_INTERVAL_LSTM_WIDTH, return_sequences=True),\n",
    "                               merge_mode='concat')(bert_exp_output)\n",
    "            p_start = Dense(1, activation='sigmoid')(\n",
    "                Concatenate(axis=-1)([bert_exp_output, M1]))\n",
    "\n",
    "            m1_tilde = Dot(axes=-2)([p_start, M1])\n",
    "            M1_tilde = Lambda(lambda x: tf.tile(\n",
    "                x, (1, MAX_SEQ_LENGTH, 1)))(m1_tilde)\n",
    "            x = Multiply()([M1, M1_tilde])\n",
    "            M2 = Bidirectional(layer=CuDNNLSTM(NUM_INTERVAL_LSTM_WIDTH, return_sequences=True),\n",
    "                               merge_mode='concat')(Concatenate(axis=-1)([bert_exp_output, M1, M1_tilde, x]))\n",
    "            p_end = Dense(1, activation='sigmoid')(\n",
    "                Concatenate(axis=-1)([bert_exp_output, M2]))\n",
    "            exp_outputs = Concatenate(\n",
    "                axis=-1, name='exp_output')([p_start, p_end])\n",
    "        outputs.append(exp_outputs)\n",
    "\n",
    "    model = Model(inputs=bert_inputs, outputs=outputs)\n",
    "    optimizer = Adam(LEARNING_RATE)\n",
    "    if par_lambda is None:\n",
    "        loss_weights = None\n",
    "    else:\n",
    "        loss_weights = {'cls_output': 1,\n",
    "                        'exp_output': par_lambda}\n",
    "    metrics = {'cls_output': 'accuracy',\n",
    "               'exp_output': [f1_wrapper(EXP_OUTPUT),\n",
    "                              precision_wrapper(EXP_OUTPUT),\n",
    "                              recall_wrapper(EXP_OUTPUT)]}\n",
    "    if EXP_OUTPUT == 'gru':\n",
    "        loss = {'cls_output': 'binary_crossentropy',\n",
    "                'exp_output': loss_function()}\n",
    "    elif EXP_OUTPUT == 'interval':\n",
    "        loss = {'cls_output': 'binary_crossentropy',\n",
    "                'exp_output': exp_interval_loss()}\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "\n",
    "    model_exp = Model(inputs=bert_inputs, outputs=exp_outputs)\n",
    "    optimizer = Adam(LEARNING_RATE)\n",
    "    model_exp.compile(loss=loss_function(), optimizer=optimizer)\n",
    "\n",
    "    return model, model_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# convert to eraser benchmark (TODO: support for interval exp output)\n",
    "from eraserbenchmark.rationale_benchmark.utils import Annotation, Evidence\n",
    "from eraserbenchmark.rationale_benchmark.utils import annotations_to_jsonl\n",
    "from bert.tokenization import FullTokenizer, BasicTokenizer, WordpieceTokenizer,\\\n",
    "    convert_to_unicode, whitespace_tokenize, convert_ids_to_tokens\n",
    "import re\n",
    "from utils import *\n",
    "from copy import deepcopy\n",
    "\n",
    "pattern = re.compile('</?(POS)?(NEG)?>')\n",
    "vocab = None\n",
    "\n",
    "\n",
    "def convert_ids_to_token_list(input_ids):\n",
    "    global vocab\n",
    "    if vocab is None:\n",
    "        from bert.tokenization import load_vocab\n",
    "        with tf.Graph().as_default():\n",
    "            bert_module = hub.Module(BERT_MODEL_HUB)\n",
    "            tokenization_info = bert_module(\n",
    "                signature=\"tokenization_info\", as_dict=True)\n",
    "            with tf.Session(config=config) as sess:\n",
    "                vocab_file = sess.run(tokenization_info[\"vocab_file\"])\n",
    "        vocab = load_vocab(vocab_file)\n",
    "\n",
    "    iv_vocab = {input_id: wordpiece for wordpiece, input_id in vocab.items()}\n",
    "    token_list = convert_ids_to_tokens(iv_vocab, input_ids)\n",
    "    return token_list\n",
    "\n",
    "\n",
    "def convert_subtoken_ids_to_tokens(ids, exps=None, raw_sentence=None):\n",
    "    subtokens = convert_ids_to_token_list(ids)\n",
    "    tokens, exps_output = [], []\n",
    "    exps_input = [0 for i in ids] if exps is None else exps\n",
    "    raw_sentence = subtokens if raw_sentence is None else raw_sentence\n",
    "    subtokens = list(\n",
    "        reversed([t[2:] if t.startswith('##') else t for t in subtokens]))\n",
    "    exps_input = list(reversed(exps_input))\n",
    "    for ref_token in raw_sentence:\n",
    "        t, e = '', 0\n",
    "        while t != ref_token and len(subtokens) > 0:\n",
    "            t += subtokens.pop()\n",
    "            e = max(e, exps_input.pop())\n",
    "        tokens.append(t)\n",
    "        exps_output.append(e)\n",
    "        if len(subtokens) == 0:\n",
    "            # the last sub-token is incomplete, ditch it directly\n",
    "            if ref_token != tokens[-1]:\n",
    "                tokens = tokens[:-1]\n",
    "                exps_output = exps_output[:-1]\n",
    "            break\n",
    "    if exps is None:\n",
    "        return tokens\n",
    "    return tokens, exps_output\n",
    "\n",
    "# [SEP] == 102\n",
    "# [CLS] == 101\n",
    "# [PAD] == 0\n",
    "\n",
    "\n",
    "def extract_texts(tokens, exps=None, text_a=True, text_b=False):\n",
    "    if tokens[0] == 101:\n",
    "        endp_text_a = tokens.index(102)\n",
    "        if text_b:\n",
    "            endp_text_b = endp_text_a + 1 + \\\n",
    "                tokens[endp_text_a + 1:].index(102)\n",
    "    else:\n",
    "        endp_text_a = tokens.index(102)\n",
    "        if text_b:\n",
    "            endp_text_b = endp_text_a + 1 + \\\n",
    "                tokens[endp_text_a + 1:].index(102)\n",
    "    ret_token = []\n",
    "    if text_a:\n",
    "        ret_token += tokens[1: endp_text_a]\n",
    "    if text_b:\n",
    "        ret_token += tokens[endp_text_a + 1: endp_text_b]\n",
    "    if exps is None:\n",
    "        return ret_token\n",
    "    else:\n",
    "        ret_exps = []\n",
    "        if text_a:\n",
    "            ret_exps += exps[1: endp_text_a]\n",
    "        if text_b:\n",
    "            ret_exps += exps[endp_text_a + 1: endp_text_b]\n",
    "        return ret_token, ret_exps\n",
    "\n",
    "\n",
    "def pred_to_exp_mask(exp_pred, count=None, threshold=0.5):\n",
    "    if count is None:\n",
    "        return (np.array(exp_pred) >= threshold).astype(np.int32)\n",
    "    temp = [(i, p) for i, p in enumerate(exp_pred)]\n",
    "    temp = sorted(temp, key=lambda x: x[1], reverse=True)\n",
    "    ret = np.zeros_like(exp_pred).astype(np.int32)\n",
    "    for i, _ in temp[:count]:\n",
    "        ret[i] = 1\n",
    "    return ret\n",
    "\n",
    "\n",
    "def rational_bits_to_ev_generator(token_list, raw_input, exp_pred, hard_selection_count, hard_selection_threshold):\n",
    "    in_rationale = False\n",
    "    ev = {'docid': raw_input['annotation_id'],\n",
    "          'start_token': -1, 'end_token': -1, 'text': ''}\n",
    "    exp_masks = pred_to_exp_mask(\n",
    "        exp_pred, hard_selection_count, hard_selection_threshold)\n",
    "    for i, p in enumerate(exp_masks):\n",
    "        if p == 0 and in_rationale:  # leave rational zone\n",
    "            in_rationale = False\n",
    "            ev['end_token'] = i\n",
    "            ev['text'] = ' '.join(\n",
    "                token_list[ev['start_token']: ev['end_token']])\n",
    "            yield deepcopy(ev)\n",
    "        elif p == 1 and not in_rationale:  # enter rational zone\n",
    "            in_rationale = True\n",
    "            ev['start_token'] = i\n",
    "    if in_rationale:  # the final non-padding token is rational\n",
    "        ev['end_token'] = len(exp_pred)\n",
    "        ev['text'] = ' '.join(token_list[ev['start_token']: ev['end_token']])\n",
    "        yield deepcopy(ev)\n",
    "\n",
    "\n",
    "def pred_to_results(raw_input, input_ids, pred, hard_selection_count, hard_selection_threshold):\n",
    "    cls_pred, exp_pred = pred\n",
    "    exp_pred = exp_pred.reshape((-1,)).tolist()\n",
    "    if 'sentence' in raw_input:\n",
    "        raw_sentence = raw_input['sentence']\n",
    "    else:\n",
    "        raw_sentence = raw_input['passage']\n",
    "    raw_sentence = re.sub(pattern, '', raw_sentence)\n",
    "    raw_sentence = re.sub('\\x12', '', raw_sentence)\n",
    "    raw_sentence = raw_sentence.lower().split()\n",
    "    if dataset == 'eraser_movie_mtl':\n",
    "        token_ids, exp_pred = extract_texts(\n",
    "            input_ids, exp_pred, text_a=True, text_b=False)\n",
    "    else:\n",
    "        token_ids, exp_pred = extract_texts(\n",
    "            input_ids, exp_pred, text_a=False, text_b=True)\n",
    "    token_list, exp_pred = convert_subtoken_ids_to_tokens(\n",
    "        token_ids, exp_pred, raw_sentence)\n",
    "    result = {'annotation_id': raw_input['annotation_id']}\n",
    "    ev_groups = []\n",
    "    result['rationales'] = [{'docid': result['annotation_id']}]\n",
    "    for ev in rational_bits_to_ev_generator(token_list, raw_input, exp_pred, hard_selection_count, hard_selection_threshold):\n",
    "        ev_groups.append(ev)\n",
    "    result['rationales'][-1]['hard_rationale_predictions'] = ev_groups\n",
    "    result['rationales'][-1]['soft_rationale_predictions'] = exp_pred + \\\n",
    "        [0] * (len(raw_sentence) - len(token_list))\n",
    "    if 'sentence' in raw_input:\n",
    "        result['classification'] = 'NEG' if round(\n",
    "            cls_pred[0]) < (NEG+POS)/2 else 'POS'\n",
    "    else:\n",
    "        result['classification'] = 'False' if round(\n",
    "            cls_pred[0]) < (NEG+POS)/2 else 'True'\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "code_folding": [],
    "colab": {},
    "colab_type": "code",
    "id": "nucD4gluYJmK",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# training, evaluation and inference\n",
    "import os\n",
    "SAVE_CHECKPOINTS_STEPS = 500\n",
    "SAVE_SUMMARY_STEPS = 100\n",
    "\n",
    "checkpoint_path = os.path.join(OUTPUT_DIR, 'cp-{epoch:04d}.ckpt')\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "train = False\n",
    "load_best = True\n",
    "assert train ^ load_best\n",
    "evaluate = False\n",
    "exp_visualize = False\n",
    "exp_benchmark = True\n",
    "cls_output_file = os.path.join(OUTPUT_DIR, 'output.txt')\n",
    "\n",
    "BENCHMARK_SPLIT_NAME = 'test'\n",
    "RES_FOR_BENCHMARK_FNAME = MODEL_NAME + '_' + BENCHMARK_SPLIT_NAME\n",
    "\n",
    "with graph.as_default():\n",
    "    set_session(sess)\n",
    "    model, model_exp = build_model(par_lambda)\n",
    "    model.summary()\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.tables_initializer())\n",
    "\n",
    "    training_inputs = [train_input_ids, train_input_masks, train_segment_ids]\n",
    "    #training_inputs = [train_input_ids[:10],\n",
    "    #                   train_input_masks[:10], train_segment_ids[:10]]\n",
    "    val_inputs = [val_input_ids, val_input_masks, val_segment_ids]\n",
    "    test_inputs = [test_input_ids, test_input_masks, test_segment_ids]\n",
    "\n",
    "    training_outputs, test_outputs, val_outputs = {}, {}, {}\n",
    "\n",
    "    if 'seq' not in dataset:\n",
    "        training_outputs['cls_output'] = train_labels\n",
    "        #training_outputs['cls_output'] = train_labels[:10]\n",
    "        test_outputs['cls_output'] = test_labels\n",
    "        val_outputs['cls_output'] = val_labels\n",
    "    if 'cls' not in dataset:\n",
    "        training_outputs['exp_output'] = train_rations\n",
    "        #training_outputs['exp_output'] = train_rations[:10]\n",
    "        test_outputs['exp_output'] = test_rations\n",
    "        val_outputs['exp_output'] = val_rations\n",
    "\n",
    "    initial_epoch = 0\n",
    "    if load_best:\n",
    "        with open(cls_output_file, 'r') as fin:\n",
    "            log = fin.readlines()\n",
    "        history = eval(log[2])\n",
    "        best_epoch = np.argmin(history['loss'])+1\n",
    "        model.load_weights(checkpoint_path.format(epoch=best_epoch))\n",
    "    else:\n",
    "        for ckpt_i in range(NUM_EPOCHS, 0, -1):\n",
    "            if os.path.isfile(checkpoint_path.format(epoch=ckpt_i)):\n",
    "                model.load_weights(checkpoint_path.format(epoch=ckpt_i))\n",
    "                assert False  # dumm proof, most of the case the training is end-to-end, without disturbance and reloading\n",
    "                break\n",
    "\n",
    "    if train:\n",
    "        cp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path,\n",
    "                                                         save_weights_only=False,\n",
    "                                                         verbose=1,\n",
    "                                                         period=1)\n",
    "        es_callback = tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "        with open(cls_output_file, 'a+') as fw:\n",
    "            fw.write(\"=============== {} ===============\\n\".format(datetime.now()))\n",
    "\n",
    "        history = model.fit(\n",
    "            training_inputs,\n",
    "            training_outputs,\n",
    "\n",
    "            validation_data=(val_inputs,\n",
    "                             val_outputs),\n",
    "            epochs=NUM_EPOCHS,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            callbacks=[cp_callback, es_callback],\n",
    "            initial_epoch=initial_epoch\n",
    "        )\n",
    "\n",
    "        with open(cls_output_file, 'a+') as fw:\n",
    "            fw.write(\"{}:\\n\".format(datetime.now()))\n",
    "            fw.write(str(history.history) + '\\n')\n",
    "\n",
    "    if evaluate:\n",
    "        evaluation_res = model.evaluate(x=test_inputs,\n",
    "                                        y=test_outputs,\n",
    "                                        batch_size=BATCH_SIZE,\n",
    "                                        verbose=1)\n",
    "        with open(cls_output_file, 'a+') as fw:\n",
    "            fw.write(\"{}:\\n\".format(datetime.now()))\n",
    "            fw.write(str(evaluation_res) + '\\n')\n",
    "\n",
    "    if exp_visualize:\n",
    "        len_head = 100\n",
    "        test_inputs_head = [x[:len_head] for x in test_inputs]\n",
    "        pred = model_exp.predict(test_inputs_head)\n",
    "        pred = np.round(np.array(pred)).astype(np.int32)\n",
    "        exp_output_folder = os.path.join(OUTPUT_DIR, 'exp_outputs/')\n",
    "        tf.gfile.MakeDirs(exp_output_folder)\n",
    "        print('marked rationals are saved under {}'.format(exp_output_folder))\n",
    "\n",
    "        def get_vocab(bert_model_hub):\n",
    "            from bert.tokenization import load_vocab\n",
    "            import tensorflow as tf\n",
    "            with tf.Graph().as_default():\n",
    "                bert_module = hub.Module(BERT_MODEL_HUB)\n",
    "                tokenization_info = bert_module(signature=\"tokenization_info\",\n",
    "                                                as_dict=True)\n",
    "                with tf.Session() as sess:\n",
    "                    vocab_file = sess.run(tokenization_info[\"vocab_file\"])\n",
    "            vocab = load_vocab(vocab_file)\n",
    "            return vocab\n",
    "\n",
    "        vocab = get_vocab(BERT_MODEL_HUB)\n",
    "\n",
    "        for i, l in enumerate(tqdm_notebook(test.iterrows())):\n",
    "            if i == len_head:\n",
    "                break\n",
    "            input_ids = test_input_ids[i]\n",
    "            pred_intp = pred[i].reshape([-1])\n",
    "            label = test_labels[i]\n",
    "            gt = test_rations[i].reshape([-1])\n",
    "            html = convert_res_to_htmls(input_ids, pred_intp, gt, vocab)\n",
    "            with open(exp_output_folder + str(l[0]) + '.html', \"w+\") as f:\n",
    "                f.write(\n",
    "                    '<h1>label: {}</h1>\\n'.format('pos' if label == 1 else 'neg'))\n",
    "                f.write(html[1] + '<br/><br/>\\n' + html[0])\n",
    "\n",
    "    if exp_benchmark:\n",
    "        result_fname = RES_FOR_BENCHMARK_FNAME + '.jsonl'\n",
    "        result_fname = os.path.join(\n",
    "            'eraserbenchmark', 'annotated_by_exp', result_fname)\n",
    "\n",
    "        if BENCHMARK_SPLIT_NAME == 'test':\n",
    "            benchmark_inputs, raw_input, benchmark_input_ids = test_inputs, test, test_input_ids\n",
    "        elif BENCHMARK_SPLIT_NAME == 'val':\n",
    "            benchmark_inputs, raw_input, benchmark_input_ids = val_inputs, val, val_input_ids\n",
    "        elif BENCHMARK_SPLIT_NAME == 'train':\n",
    "            benchmark_inputs, raw_input, benchmark_input_ids = training_inputs, train, train_input_ids\n",
    "\n",
    "        pred = model.predict(x=benchmark_inputs)\n",
    "\n",
    "        results = [pred_to_results(raw_input.loc[i], benchmark_input_ids[i], (pred[0][i], pred[1][i]), HARD_SELECTION_COUNT, HARD_SELECTION_THRESHOLD)\n",
    "                   for i in range(len(raw_input))]\n",
    "\n",
    "        def remove_rations(line, args):\n",
    "            instance_id = line.name\n",
    "            instance = args[instance_id]\n",
    "            rationales = instance['rationales'][0]['hard_rationale_predictions']\n",
    "            if 'sentence' in line:\n",
    "                sentence = line.sentence\n",
    "            else:\n",
    "                sentence = line.passage\n",
    "            sentence = re.sub(pattern, '', sentence).lower().split()\n",
    "            rationales = [{'end_token': 0, 'start_token': 0}] \\\n",
    "                + sorted(rationales, key=lambda x: x['start_token']) \\\n",
    "                + [{'start_token': len(sentence), 'end_token': len(sentence)}]\n",
    "            ret = []\n",
    "            for rat_id, rat in enumerate(rationales[:-1]):\n",
    "                ret += ['.'] * (rat['end_token'] - rat['start_token']) \\\n",
    "                    + sentence[rat['end_token']\n",
    "                        : rationales[rat_id + 1]['start_token']]\n",
    "            if 'sentence' in line:\n",
    "                line.sentence = ' '.join(ret)\n",
    "            else:\n",
    "                line.passage = ' '.join(ret)\n",
    "            return line\n",
    "\n",
    "        def extract_rations(line, args):\n",
    "            instance_id = line.name\n",
    "            instance = args[instance_id]\n",
    "            rationales = instance['rationales'][0]['hard_rationale_predictions']\n",
    "            if 'sentence' in line:\n",
    "                sentence = line.sentence\n",
    "            else:\n",
    "                sentence = line.passage\n",
    "            sentence = re.sub(pattern, '', sentence).lower().split()\n",
    "            rationales = [{'end_token': 0, 'start_token': 0}] \\\n",
    "                + sorted(rationales, key=lambda x: x['start_token']) \\\n",
    "                + [{'start_token': len(sentence), 'end_token': len(sentence)}]\n",
    "            ret = []\n",
    "            for rat_id, rat in enumerate(rationales[:-1]):\n",
    "                ret += sentence[rat['start_token']: rat['end_token']] \\\n",
    "                    + ['.'] * (rationales[rat_id + 1]\n",
    "                               ['start_token'] - rat['end_token'])\n",
    "            if 'sentence' in line:\n",
    "                line.sentence = ' '.join(ret)\n",
    "            else:\n",
    "                line.passage = ' '.join(ret)\n",
    "            return line\n",
    "\n",
    "        def get_cls_score(model, raw_input, label_list, dataset, r_function, rationales):\n",
    "            _input = deepcopy(raw_input)\n",
    "            _input = _input.apply(r_function, axis=1, args=(rationales,))\n",
    "            rets = preprocess(_input, label_list, dataset)\n",
    "            _input_ids, _input_masks, _segment_ids, _rations, _labels = rets\n",
    "\n",
    "            _inputs = [_input_ids, _input_masks, _segment_ids]\n",
    "            _pred = model.predict(_inputs)\n",
    "            return(np.hstack([1-_pred[0], _pred[0]]))\n",
    "\n",
    "        def add_cls_scores(res, cls, c, s):\n",
    "            res['classification_scores'] = {'NEG': cls[0], 'POS': cls[1]}\n",
    "            res['comprehensiveness_classification_scores'] = {\n",
    "                'NEG': c[0], 'POS': c[1]}\n",
    "            res['sufficiency_classification_scores'] = {\n",
    "                'NEG': s[0], 'POS': s[1]}\n",
    "            return res\n",
    "\n",
    "        pred_softmax = np.hstack([1-pred[0], pred[0]])\n",
    "        c_pred_softmax = get_cls_score(\n",
    "            model, raw_input, label_list, dataset, remove_rations, results)\n",
    "        s_pred_softmax = get_cls_score(\n",
    "            model, raw_input, label_list, dataset, extract_rations, results)\n",
    "\n",
    "        results = [add_cls_scores(res, cls_score, c_cls_score, s_cls_score) for res, cls_score,\n",
    "                   c_cls_score, s_cls_score in zip(results, pred_softmax, c_pred_softmax, s_pred_softmax)]\n",
    "\n",
    "        with open(result_fname+'pkl3', \"wb+\") as pfout:\n",
    "            pickle.dump(results, pfout)\n",
    "\n",
    "        from eraserbenchmark.eraser import evaluate\n",
    "        evaluate(MODEL_NAME)\n",
    "\n",
    "    with open(cls_output_file, 'a+') as fw:\n",
    "        fw.write('/////////////////experiment ends//////////////////\\n\\n\\n')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "“Predicting Movie Reviews with BERT on TF Hub.ipynb”的副本",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "493.667px",
    "left": "900px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
