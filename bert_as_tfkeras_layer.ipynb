{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext autoreload\n",
    "#%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": [],
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 183
    },
    "colab_type": "code",
    "id": "hsZvic2YxnTz",
    "outputId": "35722f7e-f064-4fb8-ecf7-c9de0b745a22"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/zzhang/workspace/interpretation_by_design/.venv/lib/python3.7/site-packages/bert/optimization.py:87: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import bert\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.python.keras.backend import set_session\n",
    "from tensorflow.python.keras.models import load_model\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from bert import run_classifier\n",
    "from bert import optimization\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "from metrices import *\n",
    "from losses import imbalanced_bce_bayesian, imbalanced_bce_resampling, exp_interval_loss\n",
    "from config import *\n",
    "from display_rational import convert_res_to_htmls\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss of the multi-task learning is:\n",
    "$$\n",
    "\\mathcal{L}_{total} = \\mathcal{L}_{cls} + \\lambda\\mathcal{L}_{exp}\\text{,}\n",
    "$$\n",
    "where $\\mathcal{L}_{cls}$ is the loss of the classification task, $\\mathcal{L}_{exp}$ is the token-wise average of explaination loss (averaged cross entropy) and $S$ indicates the length of the input text. The hyper-parameter $\\lambda$ is written as ```par_lambda``` in the following cells since 'lambda' is a reserved word in python.\n",
    "\n",
    "The loss function has been changed since 2020.01.08. The old formulation of the loss function tried to balance between cls loss and exp loss, which down-scale both term as well as their learning rate. The new schema respect the global learning rate w.r.t. cls part while one can modify the learning rate of explaination part through $\\lambda$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# hyper-parameters\n",
    "MAX_SEQ_LENGTH = 512\n",
    "HARD_SELECTION_COUNT = None\n",
    "HARD_SELECTION_THRESHOLD = 0.5\n",
    "\n",
    "BATCH_SIZE = 2   # for jupyter-notebook\n",
    "#BATCH_SIZE = 16  # for stand-alone python script\n",
    "\n",
    "#par_lambda = None\n",
    "#par_lambda = 1e-5\n",
    "par_lambda = 1\n",
    "\n",
    "NUM_EPOCHS = 3\n",
    "\n",
    "LEARNING_RATE = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "code_folding": [
     0
    ],
    "colab": {},
    "colab_type": "code",
    "id": "US_EAnICvP7f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Model output directory: model_checkpoints/bert_base_seqlen_512_eraser_movie_mtl_exp_output_gru_par_lambda_1_no_padding_imbalanced_bce_resampling_pooling_first_learning_rate_1e-05_ *****\n"
     ]
    }
   ],
   "source": [
    "# Set the output directory for saving model file\n",
    "# Optionally, set a GCP bucket location\n",
    "\n",
    "dataset = 'eraser_movie_mtl'\n",
    "#dataset = 'eraser_multirc'\n",
    "#dataset = 'eraser_fever'\n",
    "\n",
    "rebalance_approach = 'resampling'\n",
    "#rebalance_approach = 'bayesian'\n",
    "\n",
    "bert_size = 'base'\n",
    "#bert_size = 'large'\n",
    "\n",
    "if bert_size == 'base':\n",
    "    BERT_MODEL_HUB = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\"\n",
    "\n",
    "#pooling = 'mean'\n",
    "pooling = 'first'\n",
    "\n",
    "if rebalance_approach == 'resampling':\n",
    "    loss_function = imbalanced_bce_resampling\n",
    "else:\n",
    "    loss_function = imbalanced_bce_bayesian\n",
    "\n",
    "EXP_OUTPUT = 'gru'\n",
    "#EXP_OUTPUT = 'interval'\n",
    "\n",
    "suffix = ''\n",
    "#suffix = 'cls_only'\n",
    "#suffix = 'transfer_cls_to_exp'\n",
    "#suffix = 'transfer_exp_to_cls'\n",
    "\n",
    "OUTPUT_DIR = ['bert_{}_seqlen_{}_{}_exp_output_{}'.format(\n",
    "    bert_size, MAX_SEQ_LENGTH, dataset, EXP_OUTPUT)]\n",
    "DATASET_NAME = '_'.join(OUTPUT_DIR) + '_inputdata_cache'\n",
    "if par_lambda is None:\n",
    "    OUTPUT_DIR.append('no_weight')\n",
    "else:\n",
    "    OUTPUT_DIR.append('par_lambda_{}'.format(par_lambda))\n",
    "OUTPUT_DIR.append('no_padding_imbalanced_bce_{}_pooling_{}_learning_rate_{}'.format(\n",
    "    rebalance_approach, pooling, LEARNING_RATE))\n",
    "OUTPUT_DIR.append(suffix)\n",
    "\n",
    "OUTPUT_DIR = '_'.join(OUTPUT_DIR)\n",
    "MODEL_NAME = OUTPUT_DIR\n",
    "OUTPUT_DIR = os.path.join('model_checkpoints', MODEL_NAME)\n",
    "\n",
    "# @markdown Whether or not to clear/delete the directory and create a new one\n",
    "DO_DELETE = True  # @param {type:\"boolean\"}\n",
    "# @markdown Set USE_BUCKET and BUCKET if you want to (optionally) store model output on GCP bucket.\n",
    "USE_BUCKET = False  # @param {type:\"boolean\"}\n",
    "BUCKET = 'bert-base-uncased-test0'  # @param {type:\"string\"}\n",
    "\n",
    "if USE_BUCKET:\n",
    "    OUTPUT_DIR = 'gs://{}/{}'.format(BUCKET, OUTPUT_DIR)\n",
    "    from google.colab import auth\n",
    "    auth.authenticate_user()\n",
    "\n",
    "if DO_DELETE:\n",
    "    try:\n",
    "        tf.gfile.DeleteRecursively(OUTPUT_DIR)\n",
    "    except:\n",
    "        # Doesn't matter if the directory didn't exist\n",
    "        pass\n",
    "tf.gfile.MakeDirs(OUTPUT_DIR)\n",
    "print('***** Model output directory: {} *****'.format(OUTPUT_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# data loading\n",
    "if dataset == 'semeval18':\n",
    "    from load_data_semeval18 import download_and_load_datasets\n",
    "    DATA_COLUMNS = 'Tweet text'\n",
    "    LABEL_COLUMN = 'Label'\n",
    "elif dataset == 'eraser_multirc':\n",
    "    from load_data_eraser_multirc import download_and_load_datasets\n",
    "    DATA_COLUMNS = ['query', 'passage']\n",
    "    LABEL_COLUMN = 'classification'\n",
    "elif dataset == 'eraser_fever':\n",
    "    from load_data_eraser_fever import download_and_load_datasets\n",
    "    DATA_COLUMNS = ['query', 'passage']\n",
    "    LABEL_COLUMN = 'classification'\n",
    "else:\n",
    "    if dataset == 'acl_imdb' or dataset == 'acl_imdb_cls':\n",
    "        from load_data_acl_imdb import download_and_load_datasets\n",
    "    if dataset == 'semeval16' or dataset == 'semeval16_cls':\n",
    "        from load_data_semeval16 import download_and_load_datasets\n",
    "    if dataset == 'zaidan07_cls':\n",
    "        from load_data_imdb_zaidan07_cls import download_and_load_datasets\n",
    "    if dataset == 'zaidan07_seq' or dataset == 'zaidan07_mtl':\n",
    "        from load_data_imdb_zaidan07_seq import download_and_load_datasets\n",
    "    if dataset == 'eraser_movie_mtl':\n",
    "        from load_data_imdb_zaidan07_eraser import download_and_load_datasets\n",
    "    DATA_COLUMNS = ['sentence']\n",
    "    LABEL_COLUMN = 'polarity'\n",
    "\n",
    "ret = download_and_load_datasets()\n",
    "if len(ret) == 3:\n",
    "    train, val, test = ret\n",
    "else:\n",
    "    train, test = ret\n",
    "    val = train.sample(frac=0.2)\n",
    "    train = pd.merge(train, val, how='outer', indicator=True)\n",
    "    train = train.loc[train._merge == 'left_only', ['sentence', 'polarity']]\n",
    "label_list = [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# data preprocessing\n",
    "from bert_data_preprocessing_rational import load_bert_features, convert_bert_features\n",
    "\n",
    "\n",
    "def preprocess(data, label_list, dataset_name):\n",
    "    features = load_bert_features(\n",
    "        data, label_list, MAX_SEQ_LENGTH, DATA_COLUMNS, LABEL_COLUMN)\n",
    "\n",
    "    with_rations = ('cls' not in dataset_name)\n",
    "    with_lable_id = ('seq' not in dataset_name)\n",
    "\n",
    "    return convert_bert_features(features, with_lable_id, with_rations, EXP_OUTPUT)\n",
    "\n",
    "@cache_decorator(os.path.join('cache', DATASET_NAME))\n",
    "def preprocess_wrapper(*data_inputs):\n",
    "    ret = []\n",
    "    for data in data_inputs:\n",
    "        ret.append(preprocess(data, label_list, dataset))\n",
    "    return ret\n",
    "    \n",
    "rets_train, rets_val, rets_test = preprocess_wrapper(train, val, test)\n",
    "\n",
    "train_input_ids, train_input_masks, train_segment_ids, train_rations, train_labels = rets_train\n",
    "val_input_ids, val_input_masks, val_segment_ids, val_rations, val_labels = rets_val\n",
    "test_input_ids, test_input_masks, test_segment_ids, test_rations, test_labels = rets_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# initializing graph and session\n",
    "graph = tf.get_default_graph()\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# hyper-parameters of BERT's\n",
    "WARMUP_PROPORTION = 0.1\n",
    "\n",
    "num_train_steps = int(len(train_input_ids) / BATCH_SIZE * float(NUM_EPOCHS))\n",
    "num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# building models\n",
    "from model import BertLayer\n",
    "from tensorflow.keras.layers import CuDNNGRU, CuDNNLSTM\n",
    "from tensorflow.keras.layers import Bidirectional\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Reshape, Multiply, Concatenate, RepeatVector, Dot, Lambda\n",
    "\n",
    "DIM_DENSE_CLS = 256\n",
    "NUM_GRU_UNITS_BERT_SEQ = 128\n",
    "NUM_INTERVAL_LSTM_WIDTH = 100\n",
    "\n",
    "\n",
    "def build_model(par_lambda=None):\n",
    "    in_id = Input(shape=(MAX_SEQ_LENGTH,), name=\"input_ids\")\n",
    "    in_mask = Input(shape=(MAX_SEQ_LENGTH,), name=\"input_masks\")\n",
    "    in_segment = Input(shape=(MAX_SEQ_LENGTH,), name=\"segment_ids\")\n",
    "    bert_inputs = [in_id, in_mask, in_segment]\n",
    "\n",
    "    bert_cls_output, bert_exp_output = BertLayer(\n",
    "        n_fine_tune_layers=10)(bert_inputs)\n",
    "\n",
    "    outputs = []\n",
    "    if 'seq' not in dataset:\n",
    "        # Classifier output\n",
    "        dense = Dense(DIM_DENSE_CLS, activation='tanh')(bert_cls_output)\n",
    "        cls = Dense(1, activation='sigmoid', name='cls_output')(dense)\n",
    "        outputs.append(cls)\n",
    "    if 'cls' not in dataset:\n",
    "        # Explainer output\n",
    "        if EXP_OUTPUT == 'gru':\n",
    "            gru = CuDNNGRU(\n",
    "                NUM_GRU_UNITS_BERT_SEQ, kernel_initializer='random_uniform', return_sequences=True)(bert_exp_output)\n",
    "            exp = Dense(1, activation='sigmoid')(gru)\n",
    "            output_mask = Reshape((512, 1))(in_mask)\n",
    "            exp_outputs = Multiply(name='exp_output')([output_mask, exp])\n",
    "        elif EXP_OUTPUT == 'interval':\n",
    "            M1 = Bidirectional(layer=CuDNNLSTM(NUM_INTERVAL_LSTM_WIDTH, return_sequences=True),\n",
    "                               merge_mode='concat')(bert_exp_output)\n",
    "            p_start = Dense(1, activation='sigmoid')(\n",
    "                Concatenate(axis=-1)([bert_exp_output, M1]))\n",
    "\n",
    "            m1_tilde = Dot(axes=-2)([p_start, M1])\n",
    "            M1_tilde = Lambda(lambda x: tf.tile(x, (1, MAX_SEQ_LENGTH, 1)))(m1_tilde)\n",
    "            x = Multiply()([M1, M1_tilde])\n",
    "            M2 = Bidirectional(layer=CuDNNLSTM(NUM_INTERVAL_LSTM_WIDTH, return_sequences=True),\n",
    "                               merge_mode='concat')(Concatenate(axis=-1)([bert_exp_output, M1, M1_tilde, x]))\n",
    "            p_end = Dense(1, activation='sigmoid')(\n",
    "                Concatenate(axis=-1)([bert_exp_output, M2]))\n",
    "            exp_outputs = Concatenate(\n",
    "                axis=-1, name='exp_output')([p_start, p_end])\n",
    "        outputs.append(exp_outputs)\n",
    "\n",
    "    model = Model(inputs=bert_inputs, outputs=outputs)\n",
    "    optimizer = Adam(LEARNING_RATE)\n",
    "    if par_lambda is None:\n",
    "        loss_weights = None\n",
    "    else:\n",
    "        loss_weights = {'cls_output': 1,\n",
    "                        'exp_output': par_lambda}\n",
    "    metrics = {'cls_output': 'accuracy',\n",
    "               'exp_output': [f1_wrapper(EXP_OUTPUT), \n",
    "                              precision_wrapper(EXP_OUTPUT), \n",
    "                              recall_wrapper(EXP_OUTPUT)]}\n",
    "    if EXP_OUTPUT == 'gru':    \n",
    "        loss = {'cls_output': 'binary_crossentropy',\n",
    "                'exp_output': loss_function()}\n",
    "    elif EXP_OUTPUT == 'interval':\n",
    "        loss = {'cls_output': 'binary_crossentropy',\n",
    "                'exp_output': exp_interval_loss()}\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "\n",
    "    model_exp = Model(inputs=bert_inputs, outputs=exp_outputs)\n",
    "    optimizer = Adam(LEARNING_RATE)\n",
    "    model_exp.compile(loss=loss_function(), optimizer=optimizer)\n",
    "\n",
    "    return model, model_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [
     80,
     91
    ]
   },
   "outputs": [],
   "source": [
    "# convert to eraser benchmark (TODO: support for interval exp output)\n",
    "from eraserbenchmark.rationale_benchmark.utils import Annotation, Evidence\n",
    "from eraserbenchmark.rationale_benchmark.utils import annotations_to_jsonl\n",
    "from bert.tokenization import FullTokenizer, BasicTokenizer, WordpieceTokenizer,\\\n",
    "    convert_to_unicode, whitespace_tokenize, convert_ids_to_tokens\n",
    "import re\n",
    "from utils import *\n",
    "from copy import deepcopy\n",
    "\n",
    "pattern = re.compile('</?(POS)?(NEG)?>')\n",
    "vocab = None\n",
    "\n",
    "\n",
    "def convert_ids_to_token_list(input_ids):\n",
    "    global vocab\n",
    "    if vocab is None:\n",
    "        from bert.tokenization import load_vocab\n",
    "        with tf.Graph().as_default():\n",
    "            bert_module = hub.Module(BERT_MODEL_HUB)\n",
    "            tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
    "            with tf.Session() as sess:\n",
    "                vocab_file = sess.run(tokenization_info[\"vocab_file\"])\n",
    "        vocab = load_vocab(vocab_file)\n",
    "\n",
    "    iv_vocab = {input_id: wordpiece for wordpiece, input_id in vocab.items()}\n",
    "    token_list = convert_ids_to_tokens(iv_vocab, input_ids)\n",
    "    return token_list\n",
    "\n",
    "\n",
    "def convert_subtoken_ids_to_tokens(ids, exps=None, raw_sentence=None):\n",
    "    subtokens = convert_ids_to_token_list(ids)\n",
    "    tokens, exps_output = [], []\n",
    "    exps_input = [0 for i in ids] if exps is None else exps\n",
    "    raw_sentence = subtokens if raw_sentence is None else raw_sentence\n",
    "    subtokens = list(\n",
    "        reversed([t[2:] if t.startswith('##') else t for t in subtokens]))\n",
    "    exps_input = list(reversed(exps_input))\n",
    "    for ref_token in raw_sentence:\n",
    "        t, e = '', 0\n",
    "        while t != ref_token and len(subtokens) > 0:\n",
    "            t += subtokens.pop()\n",
    "            e = max(e, exps_input.pop())\n",
    "        tokens.append(t)\n",
    "        exps_output.append(e)\n",
    "        if len(subtokens) == 0:\n",
    "            if ref_token != tokens[-1]: # the last sub-token is incomplete, ditch it directly\n",
    "                tokens = tokens[:-1]\n",
    "                exps_output = exps_output[:-1]\n",
    "            break\n",
    "    if exps is None:\n",
    "        return tokens\n",
    "    return tokens, exps_output\n",
    "\n",
    "\n",
    "def extract_texts(tokens, exps=None, text_a=True, text_b=False):\n",
    "    if tokens[0] == '[CLS]':\n",
    "        endp_text_a = tokens.index('[SEP]')\n",
    "        if text_b:\n",
    "            endp_text_b = endp_text_a + 1 + \\\n",
    "                tokens[endp_text_a + 1:].index('[SEP]')\n",
    "    else:\n",
    "        endp_text_a = tokens.index('[SEP]')\n",
    "        if text_b:\n",
    "            endp_text_b = endp_text_a + 1 + tokens[endp_text_a + 1:].index('[SEP]')\n",
    "    ret_token = []\n",
    "    if text_a:\n",
    "        ret_token += tokens[1: endp_text_a]\n",
    "    if text_b:\n",
    "        ret_token += tokens[endp_text_a + 1: endp_text_b]\n",
    "    if exps is None:\n",
    "        return ret_token\n",
    "    else:\n",
    "        ret_exps = []\n",
    "        if text_a:\n",
    "            ret_exps += exps[1: endp_text_a]\n",
    "        if text_b:\n",
    "            ret_exps += exps[endp_text_a + 1: endp_text_b]\n",
    "        return ret_token, ret_exps\n",
    "\n",
    "\n",
    "def pred_to_exp_mask(exp_pred, count=None, threshold=0.5):\n",
    "    if count is None:\n",
    "        return (np.array(exp_pred) >= threshold).astype(np.int32)\n",
    "    temp = [(i, p) for i, p in enumerate(exp_pred)]\n",
    "    temp = sorted(temp, key=lambda x: x[1], reverse=True)\n",
    "    ret = np.zeros_like(exp_pred).astype(np.int32)\n",
    "    for i, _ in temp[:count]:\n",
    "        ret[i] = 1\n",
    "    return ret\n",
    "\n",
    "\n",
    "def rational_bits_to_ev_generator(token_list, raw_input, exp_pred, hard_selection_count, hard_selection_threshold):\n",
    "    in_rationale = False\n",
    "    ev = {'docid': raw_input['annotation_id'],\n",
    "          'start_token': -1, 'end_token': -1, 'text': ''}\n",
    "    exp_masks = pred_to_exp_mask(\n",
    "        exp_pred, hard_selection_count, hard_selection_threshold)\n",
    "    for i, p in enumerate(exp_masks):\n",
    "        if p == 0 and in_rationale:  # leave rational zone\n",
    "            in_rationale = False\n",
    "            ev['end_token'] = i\n",
    "            ev['text'] = ' '.join(\n",
    "                token_list[ev['start_token']: ev['end_token']])\n",
    "            yield deepcopy(ev)\n",
    "        elif p == 1 and not in_rationale:  # enter rational zone\n",
    "            in_rationale = True\n",
    "            ev['start_token'] = i\n",
    "    if in_rationale:  # the final non-padding token is rational\n",
    "        ev['end_token'] = len(exp_pred)\n",
    "        ev['text'] = ' '.join(token_list[ev['start_token']: ev['end_token']])\n",
    "        yield deepcopy(ev)\n",
    "\n",
    "\n",
    "def pred_to_results(raw_input, input_ids, pred, hard_selection_count, hard_selection_threshold):\n",
    "    cls_pred, exp_pred = pred\n",
    "    exp_pred = exp_pred.reshape((-1,)).tolist()\n",
    "    if 'sentence' in raw_input:\n",
    "        raw_sentence = raw_input['sentence']\n",
    "    else:\n",
    "        raw_sentence = raw_input['passage']\n",
    "    raw_sentence = re.sub(pattern, '', raw_sentence)\n",
    "    raw_sentence = re.sub('\\x12', '', raw_sentence)\n",
    "    raw_sentence = raw_sentence.lower().split()\n",
    "    token_ids, exp_pred = extract_texts(\n",
    "        input_ids, exp_pred, text_a=False, text_b=True)\n",
    "    token_list, exp_pred = convert_subtoken_ids_to_tokens(\n",
    "        token_ids, exp_pred, raw_sentence)\n",
    "    result = {'annotation_id': raw_input['annotation_id']}\n",
    "    ev_groups = []\n",
    "    result['rationales'] = [{'docid': result['annotation_id']}]\n",
    "    for ev in rational_bits_to_ev_generator(token_list, raw_input, exp_pred, hard_selection_count, hard_selection_threshold):\n",
    "        ev_groups.append(ev)\n",
    "    result['rationales'][-1]['hard_rationale_predictions'] = ev_groups\n",
    "    result['rationales'][-1]['soft_rationale_predictions'] = exp_pred + \\\n",
    "        [0] * (len(raw_sentence) - len(token_list))\n",
    "    if 'sentence' in raw_input:\n",
    "        result['classification'] = 'NEG' if round(\n",
    "            cls_pred[0]) < (NEG+POS)/2 else 'POS'\n",
    "    else:\n",
    "        result['classification'] = 'False' if round(\n",
    "            cls_pred[0]) < (NEG+POS)/2 else 'True'\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "cellView": "form",
    "code_folding": [
     68,
     104
    ],
    "colab": {},
    "colab_type": "code",
    "id": "nucD4gluYJmK",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method BertLayer.call of <model.BertLayer object at 0x7f8541764048>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BertLayer.call of <model.BertLayer object at 0x7f8541764048>>: AttributeError: module 'gast' has no attribute 'Num'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method BertLayer.call of <model.BertLayer object at 0x7f8541764048>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BertLayer.call of <model.BertLayer object at 0x7f8541764048>>: AttributeError: module 'gast' has no attribute 'Num'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method BertLayer.call of <model.BertLayer object at 0x7f8541764048>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BertLayer.call of <model.BertLayer object at 0x7f8541764048>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_masks (InputLayer)        [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bert_layer_2 (BertLayer)        [(None, 768), (None, 110104890   input_ids[0][0]                  \n",
      "                                                                 input_masks[0][0]                \n",
      "                                                                 segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "cu_dnngru_2 (CuDNNGRU)          (None, 512, 128)     344832      bert_layer_2[0][1]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 256)          196864      bert_layer_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)             (None, 512, 1)       0           input_masks[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 512, 1)       129         cu_dnngru_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "cls_output (Dense)              (None, 1)            257         dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "exp_output (Multiply)           (None, 512, 1)       0           reshape_2[0][0]                  \n",
      "                                                                 dense_5[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 110,646,972\n",
      "Trainable params: 71,420,802\n",
      "Non-trainable params: 39,226,170\n",
      "__________________________________________________________________________________________________\n",
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of samples seen.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of samples seen.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10 samples, validate on 200 samples\n",
      "Epoch 1/10\n",
      " 8/10 [=======================>......] - ETA: 2s - loss: 0.8790 - cls_output_loss: 0.2593 - exp_output_loss: 0.6198 - cls_output_acc: 1.0000 - exp_output_f1: 0.1070 - exp_output_precision: 0.0684 - exp_output_recall: 0.2956\n",
      "Epoch 00001: saving model to model_checkpoints/bert_base_seqlen_512_eraser_movie_mtl_exp_output_gru_par_lambda_1_no_padding_imbalanced_bce_resampling_pooling_first_learning_rate_1e-05_/cp-0001.ckpt\n",
      "10/10 [==============================] - 30s 3s/sample - loss: 0.8237 - cls_output_loss: 0.2371 - exp_output_loss: 0.5865 - cls_output_acc: 1.0000 - exp_output_f1: 0.0962 - exp_output_precision: 0.0881 - exp_output_recall: 0.2429 - val_loss: 1.5746 - val_cls_output_loss: 1.1967 - val_exp_output_loss: 0.3779 - val_cls_output_acc: 0.5000 - val_exp_output_f1: 0.0031 - val_exp_output_precision: 0.0328 - val_exp_output_recall: 0.0017\n",
      "Epoch 2/10\n",
      " 8/10 [=======================>......] - ETA: 0s - loss: 0.4417 - cls_output_loss: 0.0888 - exp_output_loss: 0.3529 - cls_output_acc: 1.0000 - exp_output_f1: 0.0000e+00 - exp_output_precision: 0.0000e+00 - exp_output_recall: 0.0000e+00\n",
      "Epoch 00002: saving model to model_checkpoints/bert_base_seqlen_512_eraser_movie_mtl_exp_output_gru_par_lambda_1_no_padding_imbalanced_bce_resampling_pooling_first_learning_rate_1e-05_/cp-0002.ckpt\n",
      "10/10 [==============================] - 9s 929ms/sample - loss: 0.4309 - cls_output_loss: 0.0829 - exp_output_loss: 0.3480 - cls_output_acc: 1.0000 - exp_output_f1: 0.0000e+00 - exp_output_precision: 0.0000e+00 - exp_output_recall: 0.0000e+00 - val_loss: 1.7660 - val_cls_output_loss: 1.5152 - val_exp_output_loss: 0.2508 - val_cls_output_acc: 0.5000 - val_exp_output_f1: 0.0000e+00 - val_exp_output_precision: 0.0000e+00 - val_exp_output_recall: 0.0000e+00\n",
      "Epoch 3/10\n",
      " 8/10 [=======================>......] - ETA: 0s - loss: 0.3551 - cls_output_loss: 0.0390 - exp_output_loss: 0.3161 - cls_output_acc: 1.0000 - exp_output_f1: 0.0000e+00 - exp_output_precision: 0.0000e+00 - exp_output_recall: 0.0000e+00\n",
      "Epoch 00003: saving model to model_checkpoints/bert_base_seqlen_512_eraser_movie_mtl_exp_output_gru_par_lambda_1_no_padding_imbalanced_bce_resampling_pooling_first_learning_rate_1e-05_/cp-0003.ckpt\n",
      "10/10 [==============================] - 10s 968ms/sample - loss: 0.3295 - cls_output_loss: 0.0386 - exp_output_loss: 0.2908 - cls_output_acc: 1.0000 - exp_output_f1: 0.0000e+00 - exp_output_precision: 0.0000e+00 - exp_output_recall: 0.0000e+00 - val_loss: 1.9700 - val_cls_output_loss: 1.7414 - val_exp_output_loss: 0.2286 - val_cls_output_acc: 0.5000 - val_exp_output_f1: 0.0000e+00 - val_exp_output_precision: 0.0000e+00 - val_exp_output_recall: 0.0000e+00\n",
      "Epoch 4/10\n",
      " 8/10 [=======================>......] - ETA: 0s - loss: 0.3240 - cls_output_loss: 0.0258 - exp_output_loss: 0.2982 - cls_output_acc: 1.0000 - exp_output_f1: 0.0000e+00 - exp_output_precision: 0.0000e+00 - exp_output_recall: 0.0000e+00\n",
      "Epoch 00004: saving model to model_checkpoints/bert_base_seqlen_512_eraser_movie_mtl_exp_output_gru_par_lambda_1_no_padding_imbalanced_bce_resampling_pooling_first_learning_rate_1e-05_/cp-0004.ckpt\n",
      "10/10 [==============================] - 10s 967ms/sample - loss: 0.3144 - cls_output_loss: 0.0242 - exp_output_loss: 0.2903 - cls_output_acc: 1.0000 - exp_output_f1: 0.0000e+00 - exp_output_precision: 0.0000e+00 - exp_output_recall: 0.0000e+00 - val_loss: 2.1836 - val_cls_output_loss: 1.9581 - val_exp_output_loss: 0.2255 - val_cls_output_acc: 0.5000 - val_exp_output_f1: 0.0000e+00 - val_exp_output_precision: 0.0000e+00 - val_exp_output_recall: 0.0000e+00\n",
      "Epoch 5/10\n",
      " 8/10 [=======================>......] - ETA: 0s - loss: 0.3166 - cls_output_loss: 0.0163 - exp_output_loss: 0.3004 - cls_output_acc: 1.0000 - exp_output_f1: 0.0000e+00 - exp_output_precision: 0.0000e+00 - exp_output_recall: 0.0000e+00\n",
      "Epoch 00005: saving model to model_checkpoints/bert_base_seqlen_512_eraser_movie_mtl_exp_output_gru_par_lambda_1_no_padding_imbalanced_bce_resampling_pooling_first_learning_rate_1e-05_/cp-0005.ckpt\n",
      "10/10 [==============================] - 9s 916ms/sample - loss: 0.2998 - cls_output_loss: 0.0164 - exp_output_loss: 0.2833 - cls_output_acc: 1.0000 - exp_output_f1: 0.0000e+00 - exp_output_precision: 0.0000e+00 - exp_output_recall: 0.0000e+00 - val_loss: 2.2858 - val_cls_output_loss: 2.0641 - val_exp_output_loss: 0.2217 - val_cls_output_acc: 0.5000 - val_exp_output_f1: 0.0000e+00 - val_exp_output_precision: 0.0000e+00 - val_exp_output_recall: 0.0000e+00\n",
      "Epoch 6/10\n",
      " 8/10 [=======================>......] - ETA: 0s - loss: 0.3085 - cls_output_loss: 0.0130 - exp_output_loss: 0.2955 - cls_output_acc: 1.0000 - exp_output_f1: 0.0000e+00 - exp_output_precision: 0.0000e+00 - exp_output_recall: 0.0000e+00\n",
      "Epoch 00006: saving model to model_checkpoints/bert_base_seqlen_512_eraser_movie_mtl_exp_output_gru_par_lambda_1_no_padding_imbalanced_bce_resampling_pooling_first_learning_rate_1e-05_/cp-0006.ckpt\n",
      "10/10 [==============================] - 9s 918ms/sample - loss: 0.2804 - cls_output_loss: 0.0128 - exp_output_loss: 0.2676 - cls_output_acc: 1.0000 - exp_output_f1: 0.0000e+00 - exp_output_precision: 0.0000e+00 - exp_output_recall: 0.0000e+00 - val_loss: 2.4083 - val_cls_output_loss: 2.1894 - val_exp_output_loss: 0.2189 - val_cls_output_acc: 0.5000 - val_exp_output_f1: 0.0000e+00 - val_exp_output_precision: 0.0000e+00 - val_exp_output_recall: 0.0000e+00\n",
      "Epoch 7/10\n",
      " 8/10 [=======================>......] - ETA: 0s - loss: 0.2284 - cls_output_loss: 0.0104 - exp_output_loss: 0.2180 - cls_output_acc: 1.0000 - exp_output_f1: 0.0000e+00 - exp_output_precision: 0.0000e+00 - exp_output_recall: 0.0000e+00\n",
      "Epoch 00007: saving model to model_checkpoints/bert_base_seqlen_512_eraser_movie_mtl_exp_output_gru_par_lambda_1_no_padding_imbalanced_bce_resampling_pooling_first_learning_rate_1e-05_/cp-0007.ckpt\n",
      "10/10 [==============================] - 9s 923ms/sample - loss: 0.2611 - cls_output_loss: 0.0106 - exp_output_loss: 0.2505 - cls_output_acc: 1.0000 - exp_output_f1: 0.0000e+00 - exp_output_precision: 0.0000e+00 - exp_output_recall: 0.0000e+00 - val_loss: 2.5097 - val_cls_output_loss: 2.2917 - val_exp_output_loss: 0.2180 - val_cls_output_acc: 0.5000 - val_exp_output_f1: 0.0000e+00 - val_exp_output_precision: 0.0000e+00 - val_exp_output_recall: 0.0000e+00\n",
      "Epoch 8/10\n",
      " 8/10 [=======================>......] - ETA: 0s - loss: 0.2793 - cls_output_loss: 0.0089 - exp_output_loss: 0.2704 - cls_output_acc: 1.0000 - exp_output_f1: 0.0000e+00 - exp_output_precision: 0.0000e+00 - exp_output_recall: 0.0000e+00\n",
      "Epoch 00008: saving model to model_checkpoints/bert_base_seqlen_512_eraser_movie_mtl_exp_output_gru_par_lambda_1_no_padding_imbalanced_bce_resampling_pooling_first_learning_rate_1e-05_/cp-0008.ckpt\n",
      "10/10 [==============================] - 11s 1s/sample - loss: 0.2502 - cls_output_loss: 0.0086 - exp_output_loss: 0.2415 - cls_output_acc: 1.0000 - exp_output_f1: 0.0000e+00 - exp_output_precision: 0.0000e+00 - exp_output_recall: 0.0000e+00 - val_loss: 2.5829 - val_cls_output_loss: 2.3640 - val_exp_output_loss: 0.2189 - val_cls_output_acc: 0.5000 - val_exp_output_f1: 0.0000e+00 - val_exp_output_precision: 0.0000e+00 - val_exp_output_recall: 0.0000e+00\n",
      "Epoch 9/10\n",
      " 8/10 [=======================>......] - ETA: 0s - loss: 0.2082 - cls_output_loss: 0.0072 - exp_output_loss: 0.2010 - cls_output_acc: 1.0000 - exp_output_f1: 0.0000e+00 - exp_output_precision: 0.0000e+00 - exp_output_recall: 0.0000e+00\n",
      "Epoch 00009: saving model to model_checkpoints/bert_base_seqlen_512_eraser_movie_mtl_exp_output_gru_par_lambda_1_no_padding_imbalanced_bce_resampling_pooling_first_learning_rate_1e-05_/cp-0009.ckpt\n",
      "10/10 [==============================] - 10s 962ms/sample - loss: 0.2357 - cls_output_loss: 0.0071 - exp_output_loss: 0.2286 - cls_output_acc: 1.0000 - exp_output_f1: 0.0094 - exp_output_precision: 0.2000 - exp_output_recall: 0.0048 - val_loss: 2.6344 - val_cls_output_loss: 2.4192 - val_exp_output_loss: 0.2152 - val_cls_output_acc: 0.5000 - val_exp_output_f1: 0.0000e+00 - val_exp_output_precision: 0.0000e+00 - val_exp_output_recall: 0.0000e+00\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 8/10 [=======================>......] - ETA: 0s - loss: 0.2414 - cls_output_loss: 0.0062 - exp_output_loss: 0.2352 - cls_output_acc: 1.0000 - exp_output_f1: 0.0312 - exp_output_precision: 0.7500 - exp_output_recall: 0.0160\n",
      "Epoch 00010: saving model to model_checkpoints/bert_base_seqlen_512_eraser_movie_mtl_exp_output_gru_par_lambda_1_no_padding_imbalanced_bce_resampling_pooling_first_learning_rate_1e-05_/cp-0010.ckpt\n",
      "10/10 [==============================] - 12s 1s/sample - loss: 0.2241 - cls_output_loss: 0.0062 - exp_output_loss: 0.2179 - cls_output_acc: 1.0000 - exp_output_f1: 0.0250 - exp_output_precision: 0.6000 - exp_output_recall: 0.0128 - val_loss: 2.6741 - val_cls_output_loss: 2.4637 - val_exp_output_loss: 0.2104 - val_cls_output_acc: 0.5000 - val_exp_output_f1: 0.0000e+00 - val_exp_output_precision: 0.0000e+00 - val_exp_output_recall: 0.0000e+00\n",
      "[1.5746407797932624, 1.766047398597002, 1.9700499008595944, 2.183647812753916, 2.285795724093914, 2.408336349129677, 2.5097106847167017, 2.5829046703875065, 2.634393502920866, 2.6740552819520236]\n",
      "1\n",
      "199/199 [==============================] - 7s 37ms/sample - loss: 1.7774 - cls_output_loss: 1.2120 - exp_output_loss: 0.5709 - cls_output_acc: 0.5025 - exp_output_f1: 0.0051 - exp_output_precision: 0.2315 - exp_output_recall: 0.0026\n"
     ]
    }
   ],
   "source": [
    "# training, evaluation and inference\n",
    "import os\n",
    "SAVE_CHECKPOINTS_STEPS = 500\n",
    "SAVE_SUMMARY_STEPS = 100\n",
    "\n",
    "NUM_EPOCHS = 10\n",
    "\n",
    "checkpoint_path = os.path.join(OUTPUT_DIR, 'cp-{epoch:04d}.ckpt')\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "evaluation = True\n",
    "inference_visualization = False\n",
    "inference_benchmark = True\n",
    "cls_output_file = os.path.join(OUTPUT_DIR, 'output.txt')\n",
    "\n",
    "BENCHMARK_SPLIT_NAME = 'test'\n",
    "RES_FOR_BENCHMARK_FNAME = MODEL_NAME + '_' + BENCHMARK_SPLIT_NAME\n",
    "\n",
    "with graph.as_default():\n",
    "    set_session(sess)\n",
    "    model, model_exp = build_model(par_lambda)\n",
    "    model.summary()\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.tables_initializer())\n",
    "    cp_callback = tf.keras.callbacks.ModelCheckpoint(checkpoint_path,\n",
    "                                                     save_weights_only=False,\n",
    "                                                     verbose=1,\n",
    "                                                     period=1)\n",
    "    es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "    initial_epoch = 0\n",
    "    for ckpt_i in range(NUM_EPOCHS, 0, -1):\n",
    "        if os.path.isfile(checkpoint_path.format(epoch=ckpt_i)):\n",
    "            initial_epoch = ckpt_i\n",
    "            model.load_weights(checkpoint_path.format(epoch=ckpt_i))\n",
    "            break\n",
    "\n",
    "    training_inputs = [train_input_ids, train_input_masks, train_segment_ids]\n",
    "    #training_inputs = [train_input_ids[:10], train_input_masks[:10], train_segment_ids[:10]]\n",
    "    val_inputs = [val_input_ids, val_input_masks, val_segment_ids]\n",
    "    test_inputs = [test_input_ids, test_input_masks, test_segment_ids]\n",
    "\n",
    "    training_outputs, test_outputs, val_outputs = {}, {}, {}\n",
    "\n",
    "    if 'seq' not in dataset:\n",
    "        training_outputs['cls_output'] = train_labels\n",
    "        #training_outputs['cls_output'] = train_labels[:10]\n",
    "        test_outputs['cls_output'] = test_labels\n",
    "        val_outputs['cls_output'] = val_labels\n",
    "    if 'cls' not in dataset:\n",
    "        training_outputs['exp_output'] = train_rations\n",
    "        #training_outputs['exp_output'] = train_rations[:10]\n",
    "        test_outputs['exp_output'] = test_rations\n",
    "        val_outputs['exp_output'] = val_rations\n",
    "\n",
    "    with open(cls_output_file, 'a+') as fw:\n",
    "        fw.write(\"=============== {} ===============\\n\".format(datetime.now()))\n",
    "\n",
    "    history = model.fit(\n",
    "        training_inputs,\n",
    "        training_outputs,\n",
    "\n",
    "        validation_data=(val_inputs,\n",
    "                         val_outputs),\n",
    "        epochs=NUM_EPOCHS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        callbacks=[cp_callback, es_callback],\n",
    "        initial_epoch=initial_epoch\n",
    "    )\n",
    "\n",
    "    with open(cls_output_file, 'a+') as fw:\n",
    "        fw.write(\"{}:\\n\".format(datetime.now()))\n",
    "        fw.write(str(history.history) + '\\n')\n",
    "\n",
    "    \n",
    "    if inference_visualization:\n",
    "        len_head = 100\n",
    "        test_inputs_head = [x[:len_head] for x in test_inputs]\n",
    "        pred = model_exp.predict(test_inputs_head)\n",
    "        pred = np.round(np.array(pred)).astype(np.int32)\n",
    "        exp_output_folder = os.path.join(OUTPUT_DIR, 'exp_outputs/')\n",
    "        tf.gfile.MakeDirs(exp_output_folder)\n",
    "        print('marked rationals are saved under {}'.format(exp_output_folder))\n",
    "\n",
    "        def get_vocab(bert_model_hub):\n",
    "            from bert.tokenization import load_vocab\n",
    "            import tensorflow as tf\n",
    "            with tf.Graph().as_default():\n",
    "                bert_module = hub.Module(BERT_MODEL_HUB)\n",
    "                tokenization_info = bert_module(signature=\"tokenization_info\",\n",
    "                                                as_dict=True)\n",
    "                with tf.Session() as sess:\n",
    "                    vocab_file = sess.run(tokenization_info[\"vocab_file\"])\n",
    "            vocab = load_vocab(vocab_file)\n",
    "            return vocab\n",
    "\n",
    "        vocab = get_vocab(BERT_MODEL_HUB)\n",
    "\n",
    "        for i, l in enumerate(tqdm_notebook(test.iterrows())):\n",
    "            if i == len_head:\n",
    "                break\n",
    "            input_ids = test_input_ids[i]\n",
    "            pred_intp = pred[i].reshape([-1])\n",
    "            label = test_labels[i]\n",
    "            gt = test_rations[i].reshape([-1])\n",
    "            html = convert_res_to_htmls(input_ids, pred_intp, gt, vocab)\n",
    "            with open(exp_output_folder + str(l[0]) + '.html', \"w+\") as f:\n",
    "                f.write(\n",
    "                    '<h1>label: {}</h1>\\n'.format('pos' if label == 1 else 'neg'))\n",
    "                f.write(html[1] + '<br/><br/>\\n' + html[0])\n",
    "\n",
    "    if inference_benchmark:\n",
    "        result_fname = RES_FOR_BENCHMARK_FNAME + '.jsonl'\n",
    "        result_fname = os.path.join('eraserbenchmark', 'results', result_fname)\n",
    "\n",
    "        if BENCHMARK_SPLIT_NAME == 'test':\n",
    "            benchmark_inputs, raw_input, benchmark_input_ids = test_inputs, test, test_input_ids\n",
    "        elif BENCHMARK_SPLIT_NAME == 'val':\n",
    "            benchmark_inputs, raw_input, benchmark_input_ids = val_inputs, val, val_input_ids\n",
    "        elif BENCHMARK_SPLIT_NAME == 'train':\n",
    "            benchmark_inputs, raw_input, benchmark_input_ids = training_inputs, train, train_input_ids\n",
    "\n",
    "        pred = model.predict(x=benchmark_inputs)\n",
    "\n",
    "        results = [pred_to_results(raw_input.loc[i], benchmark_input_ids[i], (pred[0][i], pred[1][i]), HARD_SELECTION_COUNT, HARD_SELECTION_THRESHOLD)\n",
    "                   for i in range(len(raw_input))]\n",
    "\n",
    "        def remove_rations(line, args):\n",
    "            instance_id = line.name\n",
    "            instance = args[instance_id]\n",
    "            rationales = instance['rationales'][0]['hard_rationale_predictions']\n",
    "            if 'sentence' in line:\n",
    "                sentence = line.sentence\n",
    "            else:\n",
    "                sentence = line.passage\n",
    "            sentence = re.sub(pattern, '', sentence).lower().split()\n",
    "            rationales = [{'end_token': 0}] + sorted(\n",
    "                rationales, key=lambda x: x['start_token']) + [{'start_token': len(sentence)}]\n",
    "            ret = []\n",
    "            for rat_id, rat in enumerate(rationales[:-1]):\n",
    "                ret += sentence[rat['end_token']\n",
    "                    : rationales[rat_id + 1]['start_token']]\n",
    "            if 'sentence' in line:\n",
    "                line.sentence = ' '.join(ret)\n",
    "            else:\n",
    "                line.passage = ' '.join(ret)\n",
    "            return line\n",
    "\n",
    "        def extract_rations(line, args):\n",
    "            instance_id = line.name\n",
    "            instance = args[instance_id]\n",
    "            rationales = instance['rationales'][0]['hard_rationale_predictions']\n",
    "            if 'sentence' in line:\n",
    "                sentence = line.sentence\n",
    "            else:\n",
    "                sentence = line.passage\n",
    "            sentence = re.sub(pattern, '', sentence).lower().split()\n",
    "            rationales = sorted(rationales, key=lambda x: x['start_token'])\n",
    "            ret = []\n",
    "            for rat in rationales[:-1]:\n",
    "                ret += sentence[rat['start_token']: rat['end_token']]\n",
    "            if 'sentence' in line:\n",
    "                line.sentence = ' '.join(ret)\n",
    "            else:\n",
    "                line.passage = ' '.join(ret)\n",
    "            return line\n",
    "\n",
    "        def get_cls_score(model, raw_input, label_list, dataset, r_function, rationales):\n",
    "            _input = deepcopy(raw_input)\n",
    "            _input = _input.apply(r_function, axis=1, args=(rationales,))\n",
    "            rets = preprocess(_input, label_list, dataset)\n",
    "            _input_ids, _input_masks, _segment_ids, _rations, _labels = rets\n",
    "\n",
    "            _inputs = [_input_ids, _input_masks, _segment_ids]\n",
    "            _pred = model.predict(_inputs)\n",
    "            return(np.hstack([1-_pred[0], _pred[0]]))\n",
    "\n",
    "        def add_cls_scores(res, cls, c, s):\n",
    "            res['classification_scores'] = {'NEG': cls[0], 'POS': cls[1]}\n",
    "            res['comprehensiveness_classification_scores'] = {\n",
    "                'NEG': c[0], 'POS': c[1]}\n",
    "            res['sufficiency_classification_scores'] = {\n",
    "                'NEG': s[0], 'POS': s[1]}\n",
    "            return res\n",
    "\n",
    "        pred_softmax = np.hstack([1-pred[0], pred[0]])\n",
    "        c_pred_softmax = get_cls_score(\n",
    "            model, raw_input, label_list, dataset, remove_rations, results)\n",
    "        s_pred_softmax = get_cls_score(\n",
    "            model, raw_input, label_list, dataset, extract_rations, results)\n",
    "\n",
    "        results = [add_cls_scores(res, cls_score, c_cls_score, s_cls_score) for res, cls_score,\n",
    "                   c_cls_score, s_cls_score in zip(results, pred_softmax, c_pred_softmax, s_pred_softmax)]\n",
    "\n",
    "        with open(result_fname, 'w+') as fout, open(result_fname+'pkl3', \"wb+\") as pfout:\n",
    "            fout.write(str(results))\n",
    "            pickle.dump(results, pfout)\n",
    "\n",
    "    if evaluation:\n",
    "        evaluation_res = model.evaluate(x=test_inputs,\n",
    "                                        y=test_outputs,\n",
    "                                        batch_size=BATCH_SIZE,\n",
    "                                        verbose=1)\n",
    "        with open(cls_output_file, 'a+') as fw:\n",
    "            fw.write(\"{}:\\n\".format(datetime.now()))\n",
    "            fw.write(str(evaluation_res) + '\\n')\n",
    "\n",
    "    with open(cls_output_file, 'a+') as fw:\n",
    "        fw.write('/////////////////experiment ends//////////////////\\n\\n\\n')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "“Predicting Movie Reviews with BERT on TF Hub.ipynb”的副本",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "493.667px",
    "left": "900px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
