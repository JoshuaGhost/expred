{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "143974993 MainThread Error in instances: 0 instances fail validation: set()\n",
      "143975861 MainThread No sentence level predictions detected, skipping sentence-level diagnostic\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'classification_scores': {'accuracy': 0.9246231155778895,\n",
      "                           'comprehensiveness': 0.2877288,\n",
      "                           'comprehensiveness_entropy': -0.112718515,\n",
      "                           'comprehensiveness_kl': 2.049592,\n",
      "                           'prf': {'NEG': {'f1-score': 0.9261083743842364,\n",
      "                                           'precision': 0.912621359223301,\n",
      "                                           'recall': 0.94,\n",
      "                                           'support': 100},\n",
      "                                   'POS': {'f1-score': 0.923076923076923,\n",
      "                                           'precision': 0.9375,\n",
      "                                           'recall': 0.9090909090909091,\n",
      "                                           'support': 99},\n",
      "                                   'macro avg': {'f1-score': 0.9245926487305798,\n",
      "                                                 'precision': 0.9250606796116505,\n",
      "                                                 'recall': 0.9245454545454546,\n",
      "                                                 'support': 199},\n",
      "                                   'micro avg': {'f1-score': 0.9246231155778895,\n",
      "                                                 'precision': 0.9246231155778895,\n",
      "                                                 'recall': 0.9246231155778895,\n",
      "                                                 'support': 199},\n",
      "                                   'weighted avg': {'f1-score': 0.9246002654424071,\n",
      "                                                    'precision': 0.9249981704639705,\n",
      "                                                    'recall': 0.9246231155778895,\n",
      "                                                    'support': 199}},\n",
      "                           'sufficiency': 0.17078388,\n",
      "                           'sufficiency_entropy': -0.21820341,\n",
      "                           'sufficiency_kl': 0.9291058},\n",
      " 'iou_scores': [{'macro': {'f1': 0.11686234963942344,\n",
      "                           'p': 0.13533599068603033,\n",
      "                           'r': 0.102826342435047},\n",
      "                 'micro': {'f1': 0.11174458380843785,\n",
      "                           'p': 0.13601665510062458,\n",
      "                           'r': 0.09482341557813256},\n",
      "                 'threshold': 0.5}],\n",
      " 'rationale_prf': {'instance_macro': {'f1': 0.008300613438373175,\n",
      "                                      'p': 0.013223873377583104,\n",
      "                                      'r': 0.007713693998056989},\n",
      "                   'instance_micro': {'f1': 0.00855188141391106,\n",
      "                                      'p': 0.010409437890353921,\n",
      "                                      'r': 0.00725689404934688}},\n",
      " 'token_prf': {'instance_macro': {'f1': 0.20565844115802476,\n",
      "                                  'p': 0.708957290522135,\n",
      "                                  'r': 0.13700754161730952},\n",
      "               'instance_micro': {'f1': 0.18739577415748868,\n",
      "                                  'p': 0.7699141695527782,\n",
      "                                  'r': 0.10668085461525621}},\n",
      " 'token_soft_metrics': {'auprc': 0.48046325416306934,\n",
      "                        'average_precision': 0.5023595786378573,\n",
      "                        'roc_auc_score': 0.5616735039334635}}\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import pprint\n",
    "\n",
    "from collections import Counter, defaultdict, namedtuple\n",
    "from dataclasses import dataclass\n",
    "from itertools import chain\n",
    "from typing import Any, Callable, Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from rationale_benchmark.utils import (\n",
    "    Annotation,\n",
    "    Evidence,\n",
    "    annotations_from_jsonl,\n",
    "    load_jsonl,\n",
    "    load_documents,\n",
    "    load_flattened_documents\n",
    ")\n",
    "\n",
    "from scoring import *\n",
    "from position_scored_document import PositionScoredDocument\n",
    "from verify_instances import _has_classifications, \\\n",
    "    _has_hard_predictions, \\\n",
    "    _has_soft_predictions, \\\n",
    "    _has_soft_sentence_predictions, \\\n",
    "    verify_instances\n",
    "import pickle\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG,\n",
    "                    format='%(relativeCreated)6d %(threadName)s %(message)s')\n",
    "\n",
    "dataset_name = 'movies'\n",
    "data_dir = '/home/zzhang/.keras/datasets/{}/'.format(dataset_name)\n",
    "split_name = 'test'\n",
    "\n",
    "strict = True\n",
    "strict = False\n",
    "\n",
    "iou_thresholds = [0.5]\n",
    "\n",
    "\n",
    "def evaluate(dataset_name):\n",
    "    results_fname = 'eraserbenchmark/results/{}.jsonl'.format(dataset_name)\n",
    "    score_file = 'eraserbenchmark/outputs/{}.txt'.format(dataset_name)\n",
    "    results = None\n",
    "    with open(results_fname+'pkl3', 'rb') as fin:\n",
    "        results = pickle.load(fin)\n",
    "\n",
    "    docids = set(chain.from_iterable(\n",
    "        [rat['docid'] for rat in res['rationales']] for res in results))\n",
    "    docs = load_flattened_documents(data_dir, docids)\n",
    "    verify_instances(results, docs)\n",
    "    # load truth\n",
    "    annotations = annotations_from_jsonl(\n",
    "        os.path.join(data_dir, split + '.jsonl'))\n",
    "    docids |= set(\n",
    "        chain.from_iterable(\n",
    "            (ev.docid for ev in chain.from_iterable(ann.evidences))\n",
    "            for ann in annotations\n",
    "        )\n",
    "    )\n",
    "\n",
    "    has_final_predictions = _has_classifications(results)\n",
    "    scores = dict()\n",
    "    if strict:\n",
    "        if not iou_thresholds:\n",
    "            raise ValueError(\n",
    "                \"iou_thresholds must be provided when running strict scoring\")\n",
    "        if not has_final_predictions:\n",
    "            raise ValueError(\n",
    "                \"We must have a 'classification', 'classification_score', and 'comprehensiveness_classification_score' field in order to perform scoring!\")\n",
    "\n",
    "    if _has_hard_predictions(results):\n",
    "        truth = list(chain.from_iterable(Rationale.from_annotation(ann)\n",
    "                                         for ann in annotations))\n",
    "        pred = list(chain.from_iterable(Rationale.from_instance(inst)\n",
    "                                        for inst in results))\n",
    "        if iou_thresholds is not None:\n",
    "            iou_scores = partial_match_score(truth, pred, iou_thresholds)\n",
    "            scores['iou_scores'] = iou_scores\n",
    "        # NER style scoring\n",
    "        rationale_level_prf = score_hard_rationale_predictions(truth, pred)\n",
    "        scores['rationale_prf'] = rationale_level_prf\n",
    "        token_level_truth = list(chain.from_iterable(\n",
    "            rat.to_token_level() for rat in truth))\n",
    "        token_level_pred = list(chain.from_iterable(\n",
    "            rat.to_token_level() for rat in pred))\n",
    "        token_level_prf = score_hard_rationale_predictions(\n",
    "            token_level_truth, token_level_pred)\n",
    "        scores['token_prf'] = token_level_prf\n",
    "    else:\n",
    "        logging.info(\n",
    "            \"No hard predictions detected, skipping rationale scoring\")\n",
    "\n",
    "    if _has_soft_predictions(results):\n",
    "        flattened_documents = load_flattened_documents(data_dir, docids)\n",
    "        paired_scoring = PositionScoredDocument.from_results(\n",
    "            results, annotations, flattened_documents, use_tokens=True)\n",
    "        token_scores = score_soft_tokens(paired_scoring)\n",
    "        scores['token_soft_metrics'] = token_scores\n",
    "    else:\n",
    "        logging.info(\n",
    "            \"No soft predictions detected, skipping rationale scoring\")\n",
    "\n",
    "    if _has_soft_sentence_predictions(results):\n",
    "        documents = load_documents(data_dir, docids)\n",
    "        paired_scoring = PositionScoredDocument.from_results(\n",
    "            results, annotations, documents, use_tokens=False)\n",
    "        sentence_scores = score_soft_tokens(paired_scoring)\n",
    "        scores['sentence_soft_metrics'] = sentence_scores\n",
    "    else:\n",
    "        logging.info(\n",
    "            \"No sentence level predictions detected, skipping sentence-level diagnostic\")\n",
    "\n",
    "    if has_final_predictions:\n",
    "        flattened_documents = load_flattened_documents(data_dir, docids)\n",
    "        class_results = score_classifications(\n",
    "            results, annotations, flattened_documents)\n",
    "        scores['classification_scores'] = class_results\n",
    "    else:\n",
    "        logging.info(\n",
    "            \"No classification scores detected, skipping classification\")\n",
    "\n",
    "    pprint.pprint(scores)\n",
    "\n",
    "    if score_file:\n",
    "        with open(score_file, 'w') as of:\n",
    "            json.dump(str(scores), of, indent=4, sort_keys=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
