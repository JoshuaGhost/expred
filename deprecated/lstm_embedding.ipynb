{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:61: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "WARNING:tensorflow:From /home/zzhang/workspace/interpretation_by_design/.venv/lib/python3.7/site-packages/bert/optimization.py:87: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "from utils import *\n",
    "from display_rational import convert_res_to_htmls\n",
    "from config import *\n",
    "from losses import imbalanced_bce_bayesian, imbalanced_bce_resampling, exp_interval_loss\n",
    "from metrices import *\n",
    "from tqdm import tqdm_notebook\n",
    "from bert import optimization\n",
    "from bert import run_classifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.python.keras.models import load_model\n",
    "from tensorflow.python.keras.backend import set_session\n",
    "from tensorflow.keras import backend as K\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow_hub as hub\n",
    "import os\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "\n",
    "import bert\n",
    "\n",
    "import tensorflow\n",
    "if tensorflow.__version__.startswith('2'):\n",
    "    import tensorflow.compat.v1 as tf\n",
    "    tf.disable_v2_behavior()\n",
    "else:\n",
    "    import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# parse args\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--par_lambda', type=float)\n",
    "parser.add_argument('--gpu_id', type=str)\n",
    "parser.add_argument('--batch_size', type=int)\n",
    "parser.add_argument('--num_epochs', type=int)\n",
    "parser.add_argument('--dataset', type=str, choices='fever multirc movies'.split()) # fever, multirc, movie\n",
    "parser.add_argument(\"--do_train\", action='store_true')\n",
    "parser.add_argument('--exp_visualize', action='store_true')\n",
    "parser.add_argument('--evaluate', action='store_true')\n",
    "parser.add_argument('--exp_benchmark', action='store_true')\n",
    "parser.add_argument('--exp_structure', type=str, default='gru', choices='gru rnr'.split()) # gru, rnr\n",
    "parser.add_argument('--delete_checkpoints', action='store_true')\n",
    "parser.add_argument('--merge_evidences', action='store_true')\n",
    "parser.add_argument('--benchmark_split', type=str, default='test', choices='test train val'.split()) # gru, rnr\n",
    "parser.add_argument('--train_on_portion', type=float, default=0)\n",
    "\n",
    "args = ['--par_lambda', '0.01', \n",
    "        '--gpu_id', '0', \n",
    "        '--batch_size', '2', \n",
    "        '--num_epochs', '10',\n",
    "        '--dataset', 'movies',\n",
    "        '--do_train',\n",
    "        '--evaluate',\n",
    "        '--exp_benchmark',\n",
    "        '--exp_structure', 'rnr',\n",
    "        '--delete_checkpoints',\n",
    "        '--merge_evidences']\n",
    "\n",
    "args = parser.parse_args(args)\n",
    "args = parser.parse_args()\n",
    "\n",
    "BATCH_SIZE = args.batch_size\n",
    "par_lambda = args.par_lambda\n",
    "NUM_EPOCHS = args.num_epochs\n",
    "gpu_id = args.gpu_id\n",
    "exp_structure = args.exp_structure\n",
    "dataset = args.dataset\n",
    "DO_DELETE = args.delete_checkpoints\n",
    "do_train = args.do_train\n",
    "load_best = not do_train\n",
    "evaluate = args.evaluate\n",
    "exp_visualize = args.exp_visualize\n",
    "exp_benchmark = args.exp_benchmark\n",
    "merge_evidences = args.merge_evidences\n",
    "BENCHMARK_SPLIT_NAME = args.benchmark_split\n",
    "train_on_portion = args.train_on_portion\n",
    "\n",
    "LEARNING_RATE = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings already downloaded.\n",
      "Embeddings already extracted.\n"
     ]
    }
   ],
   "source": [
    "import chakin\n",
    "\n",
    "CHAKIN_INDEX = 13\n",
    "NUMBER_OF_DIMENSIONS = 200\n",
    "SUBFOLDER_NAME = \"glove.6B\"\n",
    "\n",
    "DATA_FOLDER = \"embeddings\"\n",
    "ZIP_FILE = os.path.join(DATA_FOLDER, \"{}.zip\".format(SUBFOLDER_NAME))\n",
    "ZIP_FILE_ALT = \"glove\" + ZIP_FILE[5:]  # sometimes it's lowercase only...\n",
    "UNZIP_FOLDER = os.path.join(DATA_FOLDER, SUBFOLDER_NAME)\n",
    "if SUBFOLDER_NAME[-1] == \"d\":\n",
    "    GLOVE_FILENAME = os.path.join(UNZIP_FOLDER, \"{}.txt\".format(SUBFOLDER_NAME))\n",
    "else:\n",
    "    GLOVE_FILENAME = os.path.join(UNZIP_FOLDER, \"{}.{}d.txt\".format(SUBFOLDER_NAME, NUMBER_OF_DIMENSIONS))\n",
    "\n",
    "if not os.path.exists(ZIP_FILE) and not os.path.exists(UNZIP_FOLDER):\n",
    "    # GloVe by Stanford is licensed Apache 2.0: \n",
    "    #     https://github.com/stanfordnlp/GloVe/blob/master/LICENSE\n",
    "    #     http://nlp.stanford.edu/data/glove.twitter.27B.zip\n",
    "    #     Copyright 2014 The Board of Trustees of The Leland Stanford Junior University\n",
    "    print(\"Downloading embeddings to '{}'\".format(ZIP_FILE))\n",
    "    chakin.download(number=CHAKIN_INDEX, save_dir='./{}'.format(DATA_FOLDER))\n",
    "else:\n",
    "    print(\"Embeddings already downloaded.\")\n",
    "    \n",
    "if not os.path.exists(UNZIP_FOLDER):\n",
    "    import zipfile\n",
    "    if not os.path.exists(ZIP_FILE) and os.path.exists(ZIP_FILE_ALT):\n",
    "        ZIP_FILE = ZIP_FILE_ALT\n",
    "    with zipfile.ZipFile(ZIP_FILE,\"r\") as zip_ref:\n",
    "        print(\"Extracting embeddings to '{}'\".format(UNZIP_FOLDER))\n",
    "        zip_ref.extractall(UNZIP_FOLDER)\n",
    "else:\n",
    "    print(\"Embeddings already extracted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CACHE_DIR = 'cache/'\n",
    "DATASET_NAME = f\"lstm_attention_{dataset}_{'evidences_merged' if merge_evidences else 'evidences_splitted'}_train_on_portion_{train_on_portion}\"\n",
    "MODEL_NAME = DATASET_NAME + f'_par_lambda_{par_lambda}_lr_{LERANING_RATE}'\n",
    "DATASET_CACHE = DATASET_NAME + '_inputdata_cache'\n",
    "CHECKPOINTS_DIR = MODEL_NAME\n",
    "\n",
    "if DO_DELETE:\n",
    "    try:\n",
    "        tf.gfile.DeleteRecursively(CHECKPOINTS_DIR)\n",
    "    except:\n",
    "        pass\n",
    "if tensorflow.__version__.startswith('2'):\n",
    "    tf.io.gfile.makedirs(CHECKPOINTS_DIR)\n",
    "else:\n",
    "    tf.gfile.MakeDirs(CHECKPOINTS_DIR)\n",
    "print(f'***** Model output directory: {CHECKPOINTS_DIR} *****')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame({'docs':['this is good']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.68941421e-01, 7.31058579e-01],\n",
       "       [2.03109266e-42, 1.00000000e+00]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = np.array([[1,2], [3,99]])\n",
    "axis=-1\n",
    "y = np.exp(x - np.max(x, axis, keepdims=True))\n",
    "y / np.sum(y, axis, keepdims=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "if dataset == 'movies':\n",
    "    label_list = ['POS', 'NEG']\n",
    "elif dataset == 'multirc' or dataset == 'boolq':\n",
    "    label_list = ['True', 'False']\n",
    "elif dataset == 'fever':\n",
    "    label_list = ['SUPPORTS', 'REFUTES']\n",
    "elif dataset == ''\n",
    "        \n",
    "from utils import cache_decorator\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def load_as_df_maybe_download(dataset_name, part):\n",
    "    dataset_dir = f'~/.keras/dataset/{dataset_name}'\n",
    "    from eraserbenchmark.rationale_benchmark.utils import load_datasets, load_documents\n",
    "    \n",
    "    data_dir = f'/home/zzhang/.keras/datasets/{dataset}/'\n",
    "    train, val, test = load_datasets(data_dir)\n",
    "if train_on_portion != 0:\n",
    "    train = train[:int(len(train) * train_on_portion)]\n",
    "    if not os.path.isdir(dataset_dir):\n",
    "        import requests\n",
    "        url = f'http://www.eraserbenchmark.com/zipped/{dataset_name}.tar.gz'\n",
    "        target_path = f'{dataset_name}.tar.gz'\n",
    "        response = requests.get(url, steam=True)\n",
    "        if response.status_code == 200:\n",
    "            os.mkdir(dataset_dir)\n",
    "            with open()\n",
    "        \n",
    "        \n",
    "        \n",
    "        import requests\n",
    "\n",
    "url = 'https://pypi.python.org/packages/source/x/xlrd/xlrd-0.9.4.tar.gz'\n",
    "target_path = 'xlrd-0.9.4.tar.gz'\n",
    "\n",
    "response = requests.get(url, stream=True)\n",
    "if response.status_code == 200:\n",
    "    with open(target_path, 'wb') as f:\n",
    "        f.write(response.raw.read())\n",
    "\n",
    "@cache_decorator(os.path.join(CACHE_DIR, DATASET_NAME))\n",
    "def data_preprocess_wrapper(dataset_name, train_on_portion):\n",
    "    from copy import deepcopy\n",
    "    docs_triple, queries_triple = [], []\n",
    "    max_doc_len, max_query_len = 0, 0\n",
    "    for part in 'train val test'.split():\n",
    "        data = load_as_df_maybe_download(datset_name, part)\n",
    "        docs_triple.append(tokenize(data[DOCS_COLUMN_NAME]))\n",
    "        queries_triple.append(tokenize(data[QUERIES_COLUMN_NAME]))\n",
    "        max_doc_len = max(max_doc_len, max(list(map(len, docs_triple[-1]))))\n",
    "        max_query_len = max(max_query_len, max(list(map(len, queries_triple[-1]))))\n",
    "    ret = []\n",
    "    for docs, queries in zip(docs_triple, queries_triple):\n",
    "        docs = pad_sequences(docs, maxlen=max_doc_len)\n",
    "        queries = pad_sequences(queries, maxlen=max_query_len)\n",
    "        ret.append((docs, queries))\n",
    "    return ret, (max_doc_len, max_query_len)\n",
    "    \n",
    "(train, val, test), (max_doc_len, max_query_len) =\\\n",
    "    data_preprocess_wrapper(dataset, train_on_portion=train_on_portion)\n",
    "\n",
    "input_data = padding(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# load embeddings\n",
    "def load_embedding_from_disks(glove_filename, with_indexes=True):\n",
    "    \"\"\"\n",
    "    Read a GloVe txt file. If `with_indexes=True`, we return a tuple of two dictionnaries\n",
    "    `(word_to_index_dict, index_to_embedding_array)`, otherwise we return only a direct \n",
    "    `word_to_embedding_dict` dictionnary mapping from a string to a numpy array.\n",
    "    \"\"\"\n",
    "    if with_indexes:\n",
    "        word_to_index_dict = dict()\n",
    "        index_to_embedding_array = []\n",
    "    else:\n",
    "        word_to_embedding_dict = dict()\n",
    "\n",
    "    with open(glove_filename, 'r') as glove_file:\n",
    "        for (i, line) in enumerate(glove_file):\n",
    "            split = line.split(' ')\n",
    "            word = split[0]\n",
    "            representation = split[1:]\n",
    "            representation = np.array([float(val) for val in representation])\n",
    "            if with_indexes:\n",
    "                word_to_index_dict[word] = i\n",
    "                index_to_embedding_array.append(representation)\n",
    "            else:\n",
    "                word_to_embedding_dict[word] = representation\n",
    "\n",
    "    _WORD_NOT_FOUND = [0.0]* len(representation)  # Empty representation for unknown words.\n",
    "    if with_indexes:\n",
    "        _LAST_INDEX = i + 1\n",
    "        word_to_index_dict = defaultdict(lambda: _LAST_INDEX, word_to_index_dict)\n",
    "        index_to_embedding_array = np.array(index_to_embedding_array + [_WORD_NOT_FOUND])\n",
    "        return word_to_index_dict, index_to_embedding_array\n",
    "    else:\n",
    "        word_to_embedding_dict = defaultdict(lambda: _WORD_NOT_FOUND)\n",
    "        return word_to_embedding_dict\n",
    "    \n",
    "print(\"Loading embedding from disks...\")\n",
    "word_to_index, index_to_embedding = load_embedding_from_disks(GLOVE_FILENAME, with_indexes=True)\n",
    "print(\"Embedding loaded from disks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Embedding, Bidirectional, Attention, Dense, Concatenate, Conv2D\n",
    "# from tensorflow.keras.layers import GRU\n",
    "from tensorflow.keras.layers import CuDNNGRU as GRU\n",
    "\n",
    "GRU_DIM = 200\n",
    "\n",
    "\n",
    "doc_input = Input(shape=(MAX_DOC_LEN, ), name='doc_input')\n",
    "doc_mask_input = Input(shape=(MAX_DOC_LEN, ), name='doc_mask_input')\n",
    "\n",
    "query_input = Input(shape=(MAX_QUERY_LEN, ), name='query_input')\n",
    "query_mask_input = Input(shape=(MAX_QUERY_LEN, ), name='query_mask_input')\n",
    "\n",
    "embedding_layer = Embedding(vocab_size, embedding_dim,\n",
    "                            trainable=False, mask_zero=True, weights=[index_to_embedding])\n",
    "\n",
    "pre_H = embedding_layer(doc_input)\n",
    "pre_U = embedding_layer(query_input)\n",
    "\n",
    "H = Dropout(rate=0.2)(Bidirectional(layer=GRU(GRU_DIM, return_sequence=True),\n",
    "                  merge_mode='concat')(pre_H))  # Batch * T * 2d\n",
    "U = Dropout(rate=0.2)(Bidirectional(layer=GRU(GRU_DIM, return_sequence=True),\n",
    "                  merge_mode='concat')(pre_U))  # Batch * J * 2d\n",
    "\n",
    "H_expanded = Lambda(lambda x: tf.tile(\n",
    "    tf.expand_dims(x, 2), (1, 1, MAX_QUERY_LEN, 1)))(H)\n",
    "U_expanded = Lambda(lambda x: tf.tile(\n",
    "    tf.expand_dims(x, 1), (1, MAX_DOC_LEN, 1, 1)))(U)\n",
    "\n",
    "HUH = Concatenate(axis=-1)([H_expanded, U_expanded, H_expanded * U_expanded])\n",
    "\n",
    "Stj = Lambda(lambda x: tf.squeeze(x))(Conv2D(\n",
    "    1, (1, 1), use_bias=False, data_format='channel_last')(HUH))  # Batch * T * J\n",
    "\n",
    "U_tilde = Lambda(lambda x, y: tf.matmul(tf.nn.softmax(x, axis=2), y))(Stj, U) # T * 2d\n",
    "H_tilde = Lambda(lambda x, y: \n",
    "                 tf.transpose(\n",
    "                     tf.tile(\n",
    "                         tf.matmul(\n",
    "                             tf.transpose(\n",
    "                                 tf.nn.softmax(\n",
    "                                     tf.math.reduce_max(\n",
    "                                         x, \n",
    "                                         axis=-1,\n",
    "                                         keepdims=True\n",
    "                                     ), \n",
    "                                     axis=-2\n",
    "                                 ), \n",
    "                                 perm=(0, 2, 1)\n",
    "                             ),\n",
    "                             y\n",
    "                         ), \n",
    "                         (1, MAX_DOC_LEN, 1)\n",
    "                     ),\n",
    "                     perm=(0, 2, 1)\n",
    "                 )(Stj, H) # Batch * T * 2d\n",
    "\n",
    "                 \n",
    "G = Concatenate(axis=-1)([H, U_tilde, H * U_tilde, H * H_tilde])\n",
    "\n",
    "M1=Bidirectional(layer=CuDNNLSTM(NUM_INTERVAL_LSTM_WIDTH, return_sequences=True),\n",
    "                   merge_mode='concat')(gru2_seq)\n",
    "p_starts=Dense(1, activation='sigmoid')(\n",
    "    Concatenate(axis=-1)([concat_attention, M1]))\n",
    "\n",
    "m1_tilde=Dot(axes=-2)([p_starts, M1])\n",
    "M1_tilde=Lambda(lambda x: tf.tile(x, (1, MAX_SEQ_LENGTH, 1)))(m1_tilde)\n",
    "x=Multiply()([M1, M1_tilde])\n",
    "M2=Bidirectional(layer=CuDNNLSTM(NUM_INTERVAL_LSTM_WIDTH, return_sequences=True),\n",
    "                   merge_mode='concat')(Concatenate(axis=-1)([concat_attention, M1, M1_tilde, x]))\n",
    "p_end_given_start=Dense(MAX_SEQ_LENGTH, activation='softmax')(\n",
    "    Concatenate(axis=-1)([concat_attention, M2]))\n",
    "p_end_given_start=Lambda(\n",
    "    lambda x: tf.linalg.band_part(x, 0, -1))(p_end_given_start)\n",
    "exp_outputs=Concatenate(\n",
    "    axis=-1, name='exp_output')([p_starts, p_end_given_start])\n",
    "outputs.append(exp_outputs)\n",
    "\n",
    "if par_lambda is None:\n",
    "    loss_weights=None\n",
    "else:\n",
    "    loss_weights={'cls_output': 1,\n",
    "                    'exp_output': par_lambda}\n",
    "metrics={'cls_output': 'accuracy',\n",
    "           'exp_output': [f1_wrapper(EXP_OUTPUT),\n",
    "                          sp_precision_wrapper(EXP_OUTPUT),\n",
    "                          sp_recall_wrapper(EXP_OUTPUT),\n",
    "                          precision_wrapper(EXP_OUTPUT),\n",
    "                          recall_wrapper(EXP_OUTPUT)]}\n",
    "loss={'cls_output': 'binary_crossentropy',\n",
    "        'exp_output': loss_function()}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
