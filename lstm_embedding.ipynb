{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:61: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "WARNING:tensorflow:From /home/zzhang/workspace/interpretation_by_design/.venv/lib/python3.7/site-packages/bert/optimization.py:87: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "from utils import *\n",
    "from display_rational import convert_res_to_htmls\n",
    "from config import *\n",
    "from losses import imbalanced_bce_bayesian, imbalanced_bce_resampling, exp_interval_loss\n",
    "from metrices import *\n",
    "from tqdm import tqdm_notebook\n",
    "from bert import optimization\n",
    "from bert import run_classifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.python.keras.models import load_model\n",
    "from tensorflow.python.keras.backend import set_session\n",
    "from tensorflow.keras import backend as K\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow_hub as hub\n",
    "import os\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "\n",
    "import bert\n",
    "\n",
    "import tensorflow\n",
    "if tensorflow.__version__.startswith('2'):\n",
    "    import tensorflow.compat.v1 as tf\n",
    "    tf.disable_v2_behavior()\n",
    "else:\n",
    "    import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variable hyper-parameters\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--par_lambda', type=float)\n",
    "parser.add_argument('--gpu_id', type=str)\n",
    "parser.add_argument('--batch_size', type=int)\n",
    "parser.add_argument('--num_epochs', type=int)\n",
    "parser.add_argument('--dataset', type=str, choices='fever multirc movies'.split())\n",
    "parser.add_argument(\"--do_train\", action='store_true')\n",
    "parser.add_argument('--exp_visualize', action='store_true')\n",
    "parser.add_argument('--evaluate', action='store_true')\n",
    "parser.add_argument('--exp_benchmark', action='store_true')\n",
    "parser.add_argument('--modeling_structure', type=str, default='bert', choices='bert lstm'.split())\n",
    "parser.add_argument('--exp_structure', type=str, default='gru', choices='gru rnr'.split())\n",
    "parser.add_argument('--delete_checkpoints', action='store_true')\n",
    "parser.add_argument('--merge_evidences', action='store_true')\n",
    "\n",
    "args = ['--par_lambda', '0.01', \n",
    "        '--gpu_id', '-1', \n",
    "        '--batch_size', '2', \n",
    "        '--num_epochs', '10',\n",
    "        '--dataset', 'movies',\n",
    "        '--do_train',\n",
    "        '--evaluate',\n",
    "        '--exp_benchmark',\n",
    "        '--modeling_structure', 'lstm',\n",
    "        '--exp_structure', 'rnr',\n",
    "        '--delete_checkpoints',\n",
    "        '--merge_evidences']\n",
    "\n",
    "args = parser.parse_args(args)\n",
    "#args = parser.parse_args()\n",
    "\n",
    "BATCH_SIZE = args.batch_size\n",
    "par_lambda = args.par_lambda\n",
    "NUM_EPOCHS = args.num_epochs\n",
    "gpu_id = args.gpu_id\n",
    "exp_structure = args.exp_structure\n",
    "dataset = args.dataset\n",
    "DO_DELETE = args.delete_checkpoints\n",
    "do_train = args.do_train\n",
    "load_best = not do_train\n",
    "evaluate = args.evaluate\n",
    "exp_visualize = args.exp_visualize\n",
    "exp_benchmark = args.exp_benchmark\n",
    "merge_evidences = args.merge_evidences\n",
    "\n",
    "LEARNING_RATE = 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings already downloaded.\n",
      "Embeddings already extracted.\n"
     ]
    }
   ],
   "source": [
    "import chakin\n",
    "\n",
    "CHAKIN_INDEX = 13\n",
    "NUMBER_OF_DIMENSIONS = 200\n",
    "SUBFOLDER_NAME = \"glove.6B\"\n",
    "\n",
    "DATA_FOLDER = \"embeddings\"\n",
    "ZIP_FILE = os.path.join(DATA_FOLDER, \"{}.zip\".format(SUBFOLDER_NAME))\n",
    "ZIP_FILE_ALT = \"glove\" + ZIP_FILE[5:]  # sometimes it's lowercase only...\n",
    "UNZIP_FOLDER = os.path.join(DATA_FOLDER, SUBFOLDER_NAME)\n",
    "if SUBFOLDER_NAME[-1] == \"d\":\n",
    "    GLOVE_FILENAME = os.path.join(UNZIP_FOLDER, \"{}.txt\".format(SUBFOLDER_NAME))\n",
    "else:\n",
    "    GLOVE_FILENAME = os.path.join(UNZIP_FOLDER, \"{}.{}d.txt\".format(SUBFOLDER_NAME, NUMBER_OF_DIMENSIONS))\n",
    "\n",
    "if not os.path.exists(ZIP_FILE) and not os.path.exists(UNZIP_FOLDER):\n",
    "    # GloVe by Stanford is licensed Apache 2.0: \n",
    "    #     https://github.com/stanfordnlp/GloVe/blob/master/LICENSE\n",
    "    #     http://nlp.stanford.edu/data/glove.twitter.27B.zip\n",
    "    #     Copyright 2014 The Board of Trustees of The Leland Stanford Junior University\n",
    "    print(\"Downloading embeddings to '{}'\".format(ZIP_FILE))\n",
    "    chakin.download(number=CHAKIN_INDEX, save_dir='./{}'.format(DATA_FOLDER))\n",
    "else:\n",
    "    print(\"Embeddings already downloaded.\")\n",
    "    \n",
    "if not os.path.exists(UNZIP_FOLDER):\n",
    "    import zipfile\n",
    "    if not os.path.exists(ZIP_FILE) and os.path.exists(ZIP_FILE_ALT):\n",
    "        ZIP_FILE = ZIP_FILE_ALT\n",
    "    with zipfile.ZipFile(ZIP_FILE,\"r\") as zip_ref:\n",
    "        print(\"Extracting embeddings to '{}'\".format(UNZIP_FOLDER))\n",
    "        zip_ref.extractall(UNZIP_FOLDER)\n",
    "else:\n",
    "    print(\"Embeddings already extracted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embedding_from_disks(glove_filename, with_indexes=True):\n",
    "    \"\"\"\n",
    "    Read a GloVe txt file. If `with_indexes=True`, we return a tuple of two dictionnaries\n",
    "    `(word_to_index_dict, index_to_embedding_array)`, otherwise we return only a direct \n",
    "    `word_to_embedding_dict` dictionnary mapping from a string to a numpy array.\n",
    "    \"\"\"\n",
    "    if with_indexes:\n",
    "        word_to_index_dict = dict()\n",
    "        index_to_embedding_array = []\n",
    "    else:\n",
    "        word_to_embedding_dict = dict()\n",
    "\n",
    "    with open(glove_filename, 'r') as glove_file:\n",
    "        for (i, line) in enumerate(glove_file):\n",
    "            split = line.split(' ')\n",
    "            word = split[0]\n",
    "            representation = split[1:]\n",
    "            representation = np.array([float(val) for val in representation])\n",
    "            if with_indexes:\n",
    "                word_to_index_dict[word] = i\n",
    "                index_to_embedding_array.append(representation)\n",
    "            else:\n",
    "                word_to_embedding_dict[word] = representation\n",
    "\n",
    "    _WORD_NOT_FOUND = [0.0]* len(representation)  # Empty representation for unknown words.\n",
    "    if with_indexes:\n",
    "        _LAST_INDEX = i + 1\n",
    "        word_to_index_dict = defaultdict(lambda: _LAST_INDEX, word_to_index_dict)\n",
    "        index_to_embedding_array = np.array(index_to_embedding_array + [_WORD_NOT_FOUND])\n",
    "        return word_to_index_dict, index_to_embedding_array\n",
    "    else:\n",
    "        word_to_embedding_dict = defaultdict(lambda: _WORD_NOT_FOUND)\n",
    "        return word_to_embedding_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding from disks...\n",
      "Embedding loaded from disks.\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading embedding from disks...\")\n",
    "word_to_index, index_to_embedding = load_embedding_from_disks(GLOVE_FILENAME, with_indexes=True)\n",
    "print(\"Embedding loaded from disks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Name  Dimension                     Corpus VocabularySize  \\\n",
      "2          fastText(en)        300                  Wikipedia           2.5M   \n",
      "11         GloVe.6B.50d         50  Wikipedia+Gigaword 5 (6B)           400K   \n",
      "12        GloVe.6B.100d        100  Wikipedia+Gigaword 5 (6B)           400K   \n",
      "13        GloVe.6B.200d        200  Wikipedia+Gigaword 5 (6B)           400K   \n",
      "14        GloVe.6B.300d        300  Wikipedia+Gigaword 5 (6B)           400K   \n",
      "15       GloVe.42B.300d        300          Common Crawl(42B)           1.9M   \n",
      "16      GloVe.840B.300d        300         Common Crawl(840B)           2.2M   \n",
      "17    GloVe.Twitter.25d         25               Twitter(27B)           1.2M   \n",
      "18    GloVe.Twitter.50d         50               Twitter(27B)           1.2M   \n",
      "19   GloVe.Twitter.100d        100               Twitter(27B)           1.2M   \n",
      "20   GloVe.Twitter.200d        200               Twitter(27B)           1.2M   \n",
      "21  word2vec.GoogleNews        300          Google News(100B)           3.0M   \n",
      "\n",
      "      Method Language    Author  \n",
      "2   fastText  English  Facebook  \n",
      "11     GloVe  English  Stanford  \n",
      "12     GloVe  English  Stanford  \n",
      "13     GloVe  English  Stanford  \n",
      "14     GloVe  English  Stanford  \n",
      "15     GloVe  English  Stanford  \n",
      "16     GloVe  English  Stanford  \n",
      "17     GloVe  English  Stanford  \n",
      "18     GloVe  English  Stanford  \n",
      "19     GloVe  English  Stanford  \n",
      "20     GloVe  English  Stanford  \n",
      "21  word2vec  English    Google  \n"
     ]
    }
   ],
   "source": [
    "chakin.search(lang='English')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400001, 200)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_to_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = padding(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Embedding, Bidirectional, Attention, Dense\n",
    "from tensorflow.keras.layers import GRU\n",
    "#from tensorflow.keras.layers import CuDNNGRU as GRU\n",
    "\n",
    "GRU_DIM = 200\n",
    "\n",
    "sentence_input = Input(shape=(padded_length, ), name='input_ids')\n",
    "padding_mask_input = Input(shape=(padded_length, ), name='input_mask')\n",
    "\n",
    "embedding = Embedding(vocab_size, embedding_dim, weights=[index_to_embedding])(input_layer)\n",
    "gru1_seq = Bidirectional(layer=GRU(GRU_DIM, return_sequence=True), merge_mode='concat')(embedding)\n",
    "attention = Attention([lstm1_seq, lstm1_seq, lstm1_seq], mask=[padding_mask_input, padding_mask_input]) #padding_mask_input should be a boolean vector, element == False corresponds to the padding\n",
    "gru2_cls, gru2_seq = Bidirectional(layer=GRU(GRU_DIM, return_sequence=True, return_state=True))\n",
    "cls_output = Dense(1, activation='sigmoid')(Dropout(rate=0.05)(gru2_cls), name='cls_output')\n",
    "\n",
    "if EXP_OUTPUT == 'gru':\n",
    "    gru = CuDNNGRU(GRU_DIM, return_sequences=True)(gru2_seq)\n",
    "    exp = Dense(1, activation='sigmoid')(gru)\n",
    "    padding_mask_output = Lambda(lambda x: tf.cast(x, tf.int32))(padding_mask_input)\n",
    "    output_mask = Reshape((padded_length, 1))(padding_mask_output)\n",
    "    exp_outputs = Multiply(name='exp_output')([output_mask, exp])\n",
    "elif EXP_OUTPUT == 'rnr':\n",
    "    M1 = Bidirectional(layer=CuDNNLSTM(NUM_INTERVAL_LSTM_WIDTH, return_sequences=True),\n",
    "                       merge_mode='concat')(Concatenate(axis=-1)([bert_exp_output, ])\n",
    "    p_starts = Dense(1, activation='sigmoid')(Concatenate(axis=-1)([bert_exp_output, M1]))\n",
    "\n",
    "    m1_tilde = Dot(axes=-2)([p_starts, M1])\n",
    "    M1_tilde = Lambda(lambda x: tf.tile(x, (1, MAX_SEQ_LENGTH, 1)))(m1_tilde)\n",
    "    x = Multiply()([M1, M1_tilde])\n",
    "    M2 = Bidirectional(layer=CuDNNLSTM(NUM_INTERVAL_LSTM_WIDTH, return_sequences=True),\n",
    "                       merge_mode='concat')(Concatenate(axis=-1)([bert_exp_output, M1, M1_tilde, x]))\n",
    "    p_end_given_start = Dense(MAX_SEQ_LENGTH, activation='softmax')(Concatenate(axis=-1)([bert_exp_output, M2]))\n",
    "    p_end_given_start = Lambda(lambda x: tf.linalg.band_part(x, 0, -1))(p_end_given_start)\n",
    "    exp_outputs = Concatenate(axis=-1, name='exp_output')([p_starts, p_end_given_start])\n",
    "    #exp_outputs = Lambda(lambda x: tf.reduce_sum(x, axis=-1, keepdims=True), name='exp_output')(p_dist)\n",
    "outputs.append(exp_outputs)\n",
    "\n",
    "gru = tf.keras.layers.GRU(self.enc_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "\n",
    "       def call(self, x, hidden):\n",
    "           x = self.embedding(x)\n",
    "           output, state = self.gru(x, initial_state = hidden)\n",
    "           return output, state\n",
    "\n",
    "       def initialize_hidden_state(self):\n",
    "           return tf.zeros((self.batch_sz, self.enc_units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2  # Any size is accepted\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_VISIBILE_DIVICES'] = ''\n",
    "\n",
    "graph = tf.get_default_graph()\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "config.gpu_options.visible_device_list = \"\"\n",
    "config.graph_options.optimizer_options.global_jit_level = tf.OptimizerOptions.ON_1\n",
    "#tf.config.set_visible_devices([], 'GPU')\n",
    "#sess = tf.Session(config=config)\n",
    "#set_session(sess)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "sess = tf.InteractiveSession(config=config)  # sess = tf.Session()\n",
    "\n",
    "# Define the variable that will hold the embedding:\n",
    "tf_embedding = tf.Variable(\n",
    "    tf.constant(0.0, shape=index_to_embedding.shape),\n",
    "    trainable=False,\n",
    "    name=\"Embedding\"\n",
    ")\n",
    "\n",
    "tf_word_ids = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "\n",
    "tf_word_representation_layer = tf.nn.embedding_lookup(\n",
    "    params=tf_embedding,\n",
    "    ids=tf_word_ids\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding now stored in TensorFlow. Can delete numpy array to clear some CPU RAM.\n"
     ]
    }
   ],
   "source": [
    "tf_embedding_placeholder = tf.placeholder(tf.float32, shape=index_to_embedding.shape)\n",
    "tf_embedding_init = tf_embedding.assign(tf_embedding_placeholder)\n",
    "_ = sess.run(\n",
    "    tf_embedding_init, \n",
    "    feed_dict={\n",
    "        tf_embedding_placeholder: index_to_embedding\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"Embedding now stored in TensorFlow. Can delete numpy array to clear some CPU RAM.\")\n",
    "del index_to_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Representations for ['Hello', '!']:\n",
      "[[ 0.26609    0.21821   -0.10996   -0.48408   -0.11181   -0.09882\n",
      "  -0.45315    0.44198   -0.034614   0.10541   -0.29537   -0.10881\n",
      "   0.20916    0.52484   -0.17985   -0.31187   -0.25724    0.65267\n",
      "   0.217      0.86503    0.47239   -0.078582   0.31035   -0.12155\n",
      "  -0.12502   -0.40418    0.53803   -0.57842   -0.63668   -0.13502\n",
      "  -0.040484   0.41378   -0.63201   -0.38847   -0.43767   -0.19706\n",
      "   0.2878     0.36039   -0.032893  -0.20361   -0.34918    0.95923\n",
      "  -0.51221   -0.19035    0.1567     0.17704    0.55302    0.27636\n",
      "  -0.13707    0.91361    0.25948   -0.30107    0.48343   -0.046869\n",
      "  -0.2796    -0.040385  -0.45773    0.2768    -0.14468    0.036539\n",
      "   0.36018   -0.54939    0.19359   -0.38263   -0.29661   -0.18938\n",
      "   0.095681   0.46646    0.3366     0.78351    0.49517   -0.82418\n",
      "   0.34402   -0.50038   -0.71074   -0.25711   -0.36619    0.61746\n",
      "  -0.31281   -0.042413   0.37915   -0.62383    0.27208    0.32852\n",
      "  -0.23045   -0.12469    0.29898   -0.22525   -0.27045   -0.4447\n",
      "  -0.15889    0.20325   -0.25676   -0.80511   -0.36305    0.5591\n",
      "   0.19485   -0.087511  -0.26798   -0.020999   0.27168    0.3788\n",
      "  -0.028056  -0.31491   -0.032708  -0.037524   0.055884   0.27919\n",
      "  -0.47791    0.44201   -0.117     -0.28299    0.58407    0.1921\n",
      "  -0.27566    0.51481    0.40295    0.43387   -0.81911   -0.50214\n",
      "  -0.23985   -0.41465    0.2562    -0.2873     0.24746   -0.33388\n",
      "   0.30396    0.23779    0.24736   -0.26719   -0.59272   -0.66793\n",
      "  -0.028869   0.01017   -0.77352   -0.97084   -0.12454    0.13479\n",
      "   0.037783   0.25665   -0.12159   -0.158      0.39382   -0.40814\n",
      "   0.65089    0.10582   -0.29278   -0.36394   -0.57366    0.54263\n",
      "   0.46474    0.63384   -0.0042357  0.40399   -0.21361    0.48244\n",
      "   0.048722  -0.26775    0.077936   0.056241   0.078183  -0.14628\n",
      "  -0.27488   -0.38877   -0.10263   -0.14811   -0.20134   -0.19073\n",
      "   0.36527   -0.73402    0.35858   -0.010074   0.67942    0.65751\n",
      "  -0.048382   0.12915   -0.68121   -0.054314   0.024121   0.5411\n",
      "   1.2272     0.039207  -0.17359    0.077392  -0.14036   -0.85091\n",
      "   0.10199    0.29552    0.47807   -0.87819    0.1986    -0.073157\n",
      "  -0.23209    0.06856   -0.18215   -0.30916   -0.29031   -0.11982\n",
      "  -0.19163   -0.13518  ]\n",
      " [ 0.50368    0.7717    -0.49917   -0.11844   -0.31343    0.11234\n",
      "  -0.87627   -0.023828  -0.25441    1.0499    -0.48371   -0.26347\n",
      "   0.44609    0.18888   -0.45259    0.21814   -0.53374    0.55215\n",
      "   0.089208  -0.2118     0.54193    1.5788     0.20437   -0.0061312\n",
      "   0.2567    -0.029562  -0.085605  -0.42691   -0.32777    0.11798\n",
      "  -0.26153   -0.15132   -0.022571  -0.46733   -0.38692    0.27994\n",
      "  -0.011308  -0.59184   -0.36495    0.1691    -0.10594    0.65645\n",
      "  -0.48928    0.15948   -0.081195   0.11692    0.88431   -0.010072\n",
      "   0.24889    0.23162    0.64073    0.020086   0.43092   -0.11539\n",
      "  -0.11753    0.42164   -0.66431   -0.076698  -0.036256  -0.18624\n",
      "   0.25396   -0.73279    0.2017    -0.31605   -0.24322   -0.20399\n",
      "  -0.060672   0.45497    0.7378    -0.33076    0.13995   -0.53501\n",
      "   0.32073   -0.3237    -0.6985    -0.35884   -0.037479   0.63074\n",
      "  -0.33656    0.14987    0.99647   -0.10555    0.19376    0.54529\n",
      "  -1.0936    -0.55676    0.062936  -0.12474    0.22598   -0.72896\n",
      "   0.17856   -0.01463   -0.078208   0.19008   -0.50168   -0.1474\n",
      "  -0.092933  -0.11865   -0.39969   -0.057858   0.25717    0.67315\n",
      "   0.51202   -0.28544    0.43203   -0.5128     0.019659   1.2034\n",
      "  -0.17921    0.8853    -0.40734    0.019952   0.10009    0.24446\n",
      "  -0.65039    0.34871    0.48185    0.22524   -0.55545    0.32231\n",
      "   0.57608   -0.17042    0.19683   -0.26547    0.43111   -0.76579\n",
      "   0.24934   -0.35268    0.68673   -1.0662    -0.31459   -0.61163\n",
      "  -0.5176    -0.35523   -0.2725    -0.80081   -0.081361   0.30138\n",
      "  -0.079927   0.097338  -0.09408   -0.042193   0.592     -0.7502\n",
      "   0.99829    0.28996   -0.16991   -0.66872   -0.4085     0.16469\n",
      "   0.65422    1.0868    -0.51892    0.84679    0.65917    0.65107\n",
      "  -0.44183   -0.27975    0.27412   -0.11032    0.48424   -0.021752\n",
      "  -0.34783   -0.48813    0.1594    -0.56523    0.18211   -0.035399\n",
      "  -0.41091   -0.49701   -0.13134    0.037256   0.9695     0.188\n",
      "  -0.027763   0.03609   -0.69026   -0.16628   -0.21954    0.71331\n",
      "   1.3634    -0.33337   -0.59314    0.061749  -0.5819    -0.95098\n",
      "   0.28589    0.070689   0.23975   -1.1735    -0.31099   -0.38885\n",
      "  -0.027725  -0.46667   -0.52361   -0.42766   -0.32325   -0.29556\n",
      "  -0.50476    0.94292  ]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "batch_indexes = [word_to_index[w.lower()] for w in batch_of_words]\n",
    "\n",
    "embedding_from_batch_lookup = sess.run(\n",
    "    tf_word_representation_layer, \n",
    "    feed_dict={\n",
    "        tf_word_ids: batch_indexes\n",
    "    }\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = SUBFOLDER_NAME + \".\" + str(NUMBER_OF_DIMENSIONS) + \"d\"\n",
    "TF_EMBEDDINGS_FILE_NAME = os.path.join(DATA_FOLDER, prefix + \".ckpt\")\n",
    "DICT_WORD_TO_INDEX_FILE_NAME = os.path.join(DATA_FOLDER, prefix + \".json\")\n",
    "\n",
    "variables_to_save = [tf_embedding]\n",
    "embedding_saver = tf.train.Saver(variables_to_save)\n",
    "embedding_saver.save(sess, save_path=TF_EMBEDDINGS_FILE_NAME)\n",
    "print(\"TF embeddings saved to '{}'.\".format(TF_EMBEDDINGS_FILE_NAME))\n",
    "sess.close()\n",
    "\n",
    "with open(DICT_WORD_TO_INDEX_FILE_NAME, 'w') as f:\n",
    "    json.dump(word_to_index, f)\n",
    "print(\"word_to_index dict saved to '{}'.\".format(DICT_WORD_TO_INDEX_FILE_NAME))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
